---
ID: 3779
post_title: >
  Python网络爬虫与信息提取（第4周）知识点整理
post_name: 'python%e7%bd%91%e7%bb%9c%e7%88%ac%e8%99%ab%e4%b8%8e%e4%bf%a1%e6%81%af%e6%8f%90%e5%8f%96%ef%bc%88%e7%ac%ac4%e5%91%a8%ef%bc%89%e7%9f%a5%e8%af%86%e7%82%b9%e6%95%b4%e7%90%86'
author: 小奥
post_date: 2018-05-09 20:46:16
layout: post
link: >
  http://www.yushuai.me/2018/05/09/3779.html
published: true
tags:
  - Python
  - 网络爬虫
categories:
  - Python
---
<h1>第四周：网络爬虫之框架</h1><h2>第一讲：Scrapy爬虫框架</h2><p><strong>1.安装</strong></p><p style="text-indent:28px">执行pip install scrapy命令。</p><p style="text-indent:28px">安装后小测：执行scrapy -h</p><p><strong>2.Scrapy爬虫框架结构</strong></p><p style="text-indent:28px">爬虫框架是实现爬虫功能的一个软件结构和功能组件集合。爬虫框架是一个半成品，能够帮助用户实现专业网络爬虫。</p><p style="text-indent:28px">Scrapy爬虫包括5+2个结构，如图1所示。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180509/1525869964782267.jpg" title="1525869964782267.jpg" alt="1525869964782267.jpg" width="467" height="219"/>&nbsp;</p><p style="text-align:center">图1</p><p style="text-indent:28px">它包括三条主要的数据流路径如图中的箭头所示：</p><p style="text-indent:28px">（1）从SPIDERS发送REQUESTS到ENGINE模块，然后到SCHEDULER，SCHEDULER负责对请求进行调度。</p><p style="text-indent:28px">（2）SCHEDULER发送REQUESTS到ENGINE，再将REQUESTS传送到DOWNLOADER模块。然后DOWNLOADER返回相应通过ENGINE到SPIDERS。</p><p style="text-indent:28px">（3）从SPIDERS获取到路径（2）的RESPONSE，处理之后发送ITEMS/REQUESTS到ENGINE，然后ITEMS传递给ITEM PIPELINES，REQUESTS传递给SCHEDULER。</p><p style="text-indent:28px">这个框架的入口是SPIDERS，出口是ITEM PIPELINES。其他三个模块用户都不需要关心，用户需要编写的是SPIDERS和ITEM PIPELINES的配置。</p><p><strong>3.Scrapy爬虫框架解析</strong></p><p style="text-indent:28px">（1）ENGINE：控制所有模块之间的数据流，根据条件触发事件，不需要用户修改。</p><p style="text-indent:28px">（2）DOWNLOADER：根据请求下载网页，也不需要用户修改。</p><p style="text-indent:28px">（3）SCHEDULER，对所有爬取请求进行调度管理，不需要用户修改。</p><p style="text-indent:28px">（4）Downloader Middleware，设置目的是实施Engine、Scheduler和Downloader之间进行用户可配置的控制，可以修改、丢弃、新增请求或响应。</p><p style="text-indent:28px">（5）Spider：解析Downloader返回的响应（Response），产生爬取项（scaped item）和额外的爬取请求（Request）。</p><p style="text-indent:28px">（6）Item Pipelines：以流水线方式处理Spider产生的爬取项。它是由一组操作顺序组成，类似流水线， 操作是一个Item Pipeline类型。可能操作包括：清理、检验和查重爬取项中的HTML数据，将数据存储到数据库。</p><p style="text-indent:28px">（7）Spider Middleware：目的是对请求和爬取项进行再处理，功能包括修改、丢弃、新增请求或爬取项，用户可以配置代码。</p><p><strong>4.requests库和Scarpy爬虫比较</strong></p><table><tbody><tr class="firstRow"><td width="85" valign="top" style="border-width: 1px; border-color: windowtext; padding: 0px 7px;"><p>库名称</p></td><td width="284" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p>相同点</p></td><td width="184" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p>不同点</p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>requests</p></td><td width="284" rowspan="2" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>1.两者都可以进行页面请求和爬取。</p><p>2.两者可用性都好，文档丰富、入门简单。</p><p>3.两者都没有处理js、提交表单、应对验证码等功能（可扩展）。</p></td><td width="184" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>页面级爬虫；功能库；并发性考虑不足，性能较差；重点在于页面下载；定制灵活；上手十分简单。</p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>Scrapy</p></td><td width="184" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>网站级爬虫；框架；并发性好、性能较高；重点在于爬虫结构；一般定制灵活，深度定制困难；入门稍难。</p></td></tr></tbody></table><p>&nbsp;</p><p><strong>5.Scrapy爬虫常用命令</strong></p><p style="text-indent:28px">Scrapy是为持续运行设计的专业爬虫框架，提供操作的是Scrapy命令行。它的格式如下：</p><p>scrapy &lt;command&gt;[options][args]</p><p style="text-indent:28px">Scrapy常用命令如表1.1所示。</p><p style="text-align:center">表1.1 Scrapy常用命令</p><table><tbody><tr class="firstRow"><td width="85" valign="top" style="border-width: 1px; border-color: windowtext; padding: 0px 7px;"><p style="text-align:center">命令</p></td><td width="142" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p style="text-align:center">说明</p></td><td width="327" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p style="text-align:center">格式</p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p><strong>startproject</strong></p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>创建一个新工程</strong></p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>scrapy &nbsp; startproject &lt;name&gt; [dir]</strong></p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p><strong>genspider</strong></p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>创建一个爬虫</strong></p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>scrapy &nbsp; genspider [options] &lt;name&gt;&lt;domain&gt;</strong></p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>settings</p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>获取爬虫配置信息</p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>scrapy setting [options]</p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p><strong>crawl</strong></p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>运行一个爬虫</strong></p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p><strong>scrapy &nbsp; crawl&lt;spider&gt;</strong></p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>list</p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>列出工程中所有爬虫</p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>scrapy list</p></td></tr><tr><td width="85" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>shell</p></td><td width="142" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>启动URL调试命令行</p></td><td width="327" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>scrapy shell [url]</p></td></tr></tbody></table><p style="text-align:center">&nbsp;</p><h2>第二讲：Scrapy爬虫基本使用</h2><p><strong>1.Scrapy爬虫的第一个实例</strong></p><p>Scrapy爬虫的步骤：</p><p style="text-indent:28px">（1）第一步：建立一个工程；</p><p style="text-indent:28px">在命令行输入scrapy startproject python123demo</p><p style="text-indent:28px">生成的工程目录包括以下内容：</p><p>python123demo/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 外层目录</p><p>&nbsp;&nbsp;scrapy.cfg&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 部署Scrapy爬虫的配置文件</p><p>&nbsp; python123demo/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Scrapy框架的用户自定义Python代码</p><p style="text-indent:28px">__init__.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 初始化脚本</p><p style="text-indent:28px">items.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Items代码模板（继承类）</p><p style="text-indent:28px">middlewares.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Middlewares代码模板（继承类）</p><p style="text-indent:28px">pipelines.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Pipelines代码模板（继承类）</p><p style="text-indent:28px">settings.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 爬虫的配置文件</p><p style="text-indent:28px">spiders/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Spiders代码模板目录（继承类）</p><p style="text-indent:28px">&nbsp;&nbsp;__init__.py&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 初始文件，无需修改</p><p style="text-indent:28px">&nbsp;&nbsp;__pycache__/&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 缓存目录，无需修改</p><p style="text-indent:28px">（2）第二步：在工程中产生一个Scrapy爬虫。</p><p>scrapy genspider demo python123.io</p><p style="text-indent:28px">在demo.py文件中，parse()用于处理响应，解析内容形成字典，发现新的URL爬取请求。</p><p style="text-indent:28px">（3）第三步：配置产生的spider爬虫</p><pre class="brush:python;toolbar:false">#&nbsp;-*-&nbsp;coding:&nbsp;utf-8&nbsp;-*-
import&nbsp;scrapy
class&nbsp;DemoSpider(scrapy.Spider):
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;=&nbsp;&quot;demo&quot;
&nbsp;&nbsp;&nbsp;&nbsp;#allowed_domains&nbsp;=&nbsp;[&quot;python123.io&quot;]
&nbsp;&nbsp;&nbsp;&nbsp;start_urls&nbsp;=&nbsp;[&#39;https://python123.io/ws/demo.html&#39;]
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;parse(self,&nbsp;response):#对返回页面进行解析并且进行操作的相关步骤
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;fname&nbsp;=&nbsp;response.url.split(&#39;/&#39;)[-1]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;open(fname,&nbsp;&#39;wb&#39;)&nbsp;as&nbsp;f:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;f.write(response.body)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.log(&#39;Saved&nbsp;file&nbsp;%s.&#39;&nbsp;%&nbsp;name)</pre><p style="text-indent:28px">（4）运行爬虫，获取网页。</p><pre class="brush:python;toolbar:false">scrapy&nbsp;crawl&nbsp;demo</pre><p><strong>2.yield关键字的使用</strong></p><p style="text-align:center"><strong><span style="font-size: 20px;">yield&lt;---&gt;生成器</span></strong></p><p style="text-indent:28px">生成器是一个不断产生值的函数。</p><p style="text-indent:28px">包含yield语句的函数是一个生成器。</p><p style="text-indent:28px">生成器每次产生一个值（yield语句），函数被冻结，被唤醒后再产生一个值。</p><p style="text-indent:28px">实例：</p><pre class="brush:python;toolbar:false">def&nbsp;gen(n):
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(n):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;i**2
for&nbsp;i&nbsp;in&nbsp;gen(5):
print(i,&quot;&nbsp;&quot;,end=&quot;&quot;)</pre><p style="text-indent:28px">生成器相比一次列出所有内容的优势：更节省存储空间；响应更迅速；使用更加灵活。</p><p><strong>3.Scrapy爬虫的基本使用</strong></p><p>Scrapy爬虫的使用步骤：</p><p style="text-indent:28px">步骤1：创建一个工程和Spider模板</p><p style="text-indent:28px">步骤2：编写Spider</p><p style="text-indent:28px">步骤3：编写Item Pipeline</p><p style="text-indent:28px">步骤4：优化配置策略</p><p>Scrapy爬虫的数据类型：</p><p style="text-indent:28px">Request类：class scrapy.http.Request()</p><p style="text-indent:28px">Request对象表示一个HTTP请求。由Spider生成，由Donwnloader执行。它包括6个属性或方法：</p><p style="text-align:center">表2.1 Request类的属性或方法</p><table><tbody><tr class="firstRow"><td width="94" valign="top" style="border-width: 1px; border-color: windowtext; padding: 0px 7px;"><p style="text-align:center">属性或方法</p></td><td width="459" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p style="text-align:center">说明</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.url</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Request对应的请求url地址</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.method</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>对应的请求方法，‘GET’’POST’等</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.headers</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>字典类型风格的请求头</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.body</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>请求内容主题，字符串类型</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.meta</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>用户添加的扩展信息，在Scrapy内部模板间传递信息使用</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.copy()</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>复制该请求</p></td></tr></tbody></table><p style="text-align:center">&nbsp;</p><p style="text-indent:28px">Response类：class scrapy.http.Response()</p><p style="text-indent:28px">Response对象表示一个HTTP响应，由Downloader生成，Spider处理。</p><p style="text-align:center">表2.2 Response类的属性或方法</p><table><tbody><tr class="firstRow"><td width="94" valign="top" style="border-width: 1px; border-color: windowtext; padding: 0px 7px;"><p style="text-align:center">属性或方法</p></td><td width="459" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p style="text-align:center">说明</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.url</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Response对应的请求url地址</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.status</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>HTTP状态码，默认是200</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.headers</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Response对应的头部信息</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.body</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Response对应的内容信息，字符串类型</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.flags</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>一组标记</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.request</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>产生Response类型对应的Request对象</p></td></tr><tr><td width="94" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>.copy()</p></td><td width="459" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>复制该响应</p></td></tr></tbody></table><p>&nbsp;</p><p style="text-indent:28px">Item类：class scrapy.item.Item()</p><p style="text-indent:28px">Item对象表示一个从HTML页面中提取的信息内容。由Spider生成，由Item Pipeline处理。Item类似字典类型，可以按照字典类型操作。</p><p style="text-indent:28px">Scrapy爬虫支持多种HTML信息提取方法，包括Beautiful Soup/lxml/re/XPath Selector/CSS Seletor等。</p><p style="text-indent:28px">下面简单介绍一下CSS Selector。CSS Selector的基本使用格式如下：</p><pre class="brush:python;toolbar:false">&lt;HTML&gt;.css(‘a::attr(href)’).extract()</pre><p style="text-indent:28px">其中a是标签名称，href是标签属性，这样就能获得对应的标签信息。CSS Selector是由W3C组织维护并规范。</p><h2>第三讲：实例4：股票数据Scrapy爬虫</h2><p><strong>1.实例介绍</strong></p><p>技术路线：scrapy</p><p>目标：获取上交所和深交所所有股票的名称和交易信息。</p><p>输出：保存在文件中。</p><p><strong>2.实例编写</strong></p><p style="text-indent:28px">（1）配置stocks.py文件：修改对返回页面的处理；修改对新增URL爬取请求的处理。</p><p>stocks.py代码</p><pre class="brush:python;toolbar:false">#&nbsp;-*-&nbsp;coding:&nbsp;utf-8&nbsp;-*-
import&nbsp;scrapy
import&nbsp;re
class&nbsp;StocksSpider(scrapy.Spider):
&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;=&nbsp;&quot;stocks&quot;
&nbsp;&nbsp;&nbsp;&nbsp;start_urls&nbsp;=&nbsp;[&#39;https://quote.eastmoney.com/stocklist.html&#39;]
&nbsp;
def&nbsp;parse(self,&nbsp;response):
#对页面中所有的a链接进行提取，格式如下所示
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;href&nbsp;in&nbsp;response.css(&#39;a::attr(href)&#39;).extract():
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stock&nbsp;=&nbsp;re.findall(r&quot;[s][hz]\d{6}&quot;,&nbsp;href)[0]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;url&nbsp;=&nbsp;&#39;https://gupiao.baidu.com/stock/&#39;&nbsp;+&nbsp;stock&nbsp;+&nbsp;&#39;.html&#39;
#上面是利用东方财富网中获取的股票代码在百度股票中获取其信息
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;scrapy.Request(url,&nbsp;callback=self.parse_stock)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;continue
#如下在百度股票的单个页面中提取股票所需信息
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;parse_stock(self,&nbsp;response):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;infoDict&nbsp;=&nbsp;{}
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stockInfo&nbsp;=&nbsp;response.css(&#39;.stock-bets&#39;)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name&nbsp;=&nbsp;stockInfo.css(&#39;.bets-name&#39;).extract()[0]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;keyList&nbsp;=&nbsp;stockInfo.css(&#39;dt&#39;).extract()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;valueList&nbsp;=&nbsp;stockInfo.css(&#39;dd&#39;).extract()
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(len(keyList)):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;key&nbsp;=&nbsp;re.findall(r&#39;&gt;.*&lt;/dt&gt;&#39;,&nbsp;keyList[i])[0][1:-5]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val&nbsp;=&nbsp;re.findall(r&#39;\d+\.?.*&lt;/dd&gt;&#39;,&nbsp;valueList[i])[0][0:-5]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;val&nbsp;=&nbsp;&#39;--&#39;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;infoDict[key]=val
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;infoDict.update(
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{&#39;股票名称&#39;:&nbsp;re.findall(&#39;\s.*\(&#39;,name)[0].split()[0]&nbsp;+&nbsp;\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;re.findall(&#39;\&gt;.*\&lt;&#39;,&nbsp;name)[0][1:-1]})
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;infoDict</pre><p>&nbsp;&nbsp;&nbsp; （2）编写Pipelines，配置pipelines.py文件，定义对爬取项（Scraped Item）的处理类，配置ITEM_PIPELINES选项。</p><p>pipelines.py源代码：</p><pre class="brush:python;toolbar:false">#&nbsp;-*-&nbsp;coding:&nbsp;utf-8&nbsp;-*-
#&nbsp;Define&nbsp;your&nbsp;item&nbsp;pipelines&nbsp;here
#&nbsp;Don&#39;t&nbsp;forget&nbsp;to&nbsp;add&nbsp;your&nbsp;pipeline&nbsp;to&nbsp;the&nbsp;ITEM_PIPELINES&nbsp;setting
#&nbsp;See:&nbsp;https://doc.scrapy.org/en/latest/topics/item-pipeline.html
class&nbsp;BaidustocksPipeline(object):
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;process_item(self,&nbsp;item,&nbsp;spider):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;item
&nbsp;
class&nbsp;BaidustocksInfoPipeline(object):
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;open_spider(self,&nbsp;spider):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.f&nbsp;=&nbsp;open(&#39;BaiduStockInfo.txt&#39;,&nbsp;&#39;w&#39;)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;close_spider(self,&nbsp;spider):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.f.close()
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;process_item(self,&nbsp;item,&nbsp;spider):#对每一个item进行处理
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;try:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;line&nbsp;=&nbsp;str(dict(item))&nbsp;+&nbsp;&#39;\n&#39;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;self.f.write(line)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;except:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;item</pre><p style="text-indent:28px">setting.py中被修改的部分的代码：</p><pre class="brush:python;toolbar:false">#&nbsp;Configure&nbsp;item&nbsp;pipelines
#&nbsp;See&nbsp;https://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html
ITEM_PIPELINES&nbsp;=&nbsp;{
&nbsp;&nbsp;&nbsp;&nbsp;&#39;BaiduStocks.pipelines.BaidustocksInfoPipeline&#39;:&nbsp;300,
}</pre><p><strong>3.实例优化</strong></p><p style="text-indent:28px">配置并发连接选项，在settings.py中，具体如下：</p><p style="text-align:center">表3.1 settings.py文件</p><table><tbody><tr class="firstRow"><td width="217" valign="top" style="border-width: 1px; border-color: windowtext; padding: 0px 7px;"><p style="text-align:center">选项</p></td><td width="336" valign="top" style="border-top-width: 1px; border-right-width: 1px; border-bottom-width: 1px; border-top-color: windowtext; border-right-color: windowtext; border-bottom-color: windowtext; border-left: none; padding: 0px 7px;"><p style="text-align:center">说明</p></td></tr><tr><td width="217" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>CONCURENT_REQUESTS</p></td><td width="336" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Downloader最大并发请求下载数量，默认32</p></td></tr><tr><td width="217" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>CONCURENT_ITEMS</p></td><td width="336" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>Item Pipeline最大并发ITEM处理数量，默认100</p></td></tr><tr><td width="217" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p><span style="font-size:13px">CONCURENT_REQUESTS_PER_DOMAIN</span></p></td><td width="336" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>每个目标域名最大的并发请求数量，默认为8</p></td></tr><tr><td width="217" valign="top" style="border-right-width: 1px; border-bottom-width: 1px; border-left-width: 1px; border-right-color: windowtext; border-bottom-color: windowtext; border-left-color: windowtext; border-top: none; padding: 0px 7px;"><p>CONCURENT_REQUESTS_PER_IP</p></td><td width="336" valign="top" style="border-top: none; border-left: none; border-bottom-width: 1px; border-bottom-color: windowtext; border-right-width: 1px; border-right-color: windowtext; padding: 0px 7px;"><p>每个目标IP最大的并发请求数量，默认0，非0有效</p></td></tr></tbody></table><p style="text-align:center">&nbsp;</p><p><br/></p>