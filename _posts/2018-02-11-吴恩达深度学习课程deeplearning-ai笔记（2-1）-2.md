---
ID: 3617
post_title: >
  吴恩达深度学习课程DeepLearning.ai笔记（2-2）
post_name: '%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8bdeeplearning-ai%e7%ac%94%e8%ae%b0%ef%bc%882-1%ef%bc%89-2'
author: 小奥
post_date: 2018-02-11 14:30:58
layout: post
link: >
  http://www.yushuai.me/2018/02/11/3617.html
published: true
tags:
  - 人工智能
  - 吴恩达
  - 神经网络
categories:
  - Deep Learning
---
<p style="margin-top: 8px;background: white"><strong><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">1. Mini-batch </span></strong><strong><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">梯度下降法</span></strong></p><p style="text-indent:32px;background:white"><span style="font-family: 微软雅黑, sans-serif; color: #4F4F4F; font-size: 14px;">对整个训练集进行梯度下降法的时候，我们必须处理整个训练数据集，然后才能进行一步梯度下降，即每一步梯度下降法需要对整个训练集进行一次处理，如果训练数据集很大的时候，处理速度就会非常慢。如果我们每次训练的时候只训练一小部分，则我们的算法速度会执行的更快。而处理的这些一小部分训练子集即称为Mini-batch。</span></p><p><span style="font-size: 14px;">整个算法的过程如图所示：</span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518336484130087.jpg" title="1518336484130087.jpg" alt="1518336484130087.jpg" width="670" height="379"/></p><p style="text-align: center;">图1</p><p><span style="font-size: 14px;">接下来对不同size情况下性能进行比较：</span></p><p style="margin-top: 8px;background: white"><span style="font-family: 微软雅黑, sans-serif; color: #454545; font-size: 14px;">（1）batch梯度下降：&nbsp;</span></p><p style="margin: 8px 0 0 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">对所有m个训练样本执行一次梯度下降，每一次迭代时间较长；</span></span></p><p style="margin: 8px 0 0 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">Cost function 总是向减小的方向下降。</span></span></p><p style="margin-top: 8px;background: white"><span style="font-family: 微软雅黑, sans-serif; color: #454545; font-size: 14px;">（2）随机梯度下降：&nbsp;</span></p><p style="margin: 8px 0 0 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">对每一个训练样本执行一次梯度下降，但是丢失了向量化带来的计算加速；</span></span></p><p style="margin: 8px 0 0 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">Cost function总体的趋势向最小值的方向下降，但是无法到达全局最小值点，呈现波动的形式。</span></span></p><p style="margin-top: 8px;background: white"><span style="font-family: 微软雅黑, sans-serif; color: #454545; font-size: 14px;">（3）Mini-batch梯度下降：&nbsp;<br/> <br/> </span></p><p style="margin-left: 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">选择一个</span><span style="font-size: 20px; font-family: MathJax_Main, serif; color: #454545;">1&lt;</span><span style="font-size: 20px; font-family: MathJax_Math-italic, serif; color: #454545;">size</span><span style="font-size: 20px; font-family: MathJax_Main, serif; color: #454545;">&lt;</span><span style="font-size: 20px; font-family: MathJax_Math-italic, serif; color: #454545;">m</span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">&nbsp;的合适的size进行Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处。</span></span></p><p style="margin: 8px 0 0 64px;background: white"><span style="font-size: 14px;"><span style="font-size: 13px; font-family: Symbol; color: #454545;">·<span style="font-variant-numeric: normal; font-stretch: normal; font-size: 9px; line-height: normal; font-family: &quot;Times New Roman&quot;;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-size: 16px; font-family: 微软雅黑, sans-serif; color: #454545;">Cost function的下降处于前两者之间。</span></span></p><p>Mini-batch大小选择原则：</p><p>（1）如果训练集比较小（≤2000），可以采用batch算法。</p><p>（2）训练集比较大（如）时可采用mini-batch。</p><p>（3）mini-batch选择应该符合CPU/GPU的性能要求。</p><h3 style="margin-top:8px;margin-right:0;margin-bottom:0;margin-left: 0;margin-bottom:0;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">2. 指数加权平均</span></h3><p style=";text-align: justify;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">指数加权平均的关键函数：</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">设当前参数为：</span><span style="font-family:等线">θ</span><sub>1</sub>,<span style="font-family:等线">θ</span><sub>2</sub>,…, <span style="font-family:等线">θ</span><sub>t</sub>，则：</p><p style="text-align:center">v<sub>t</sub>=βv<sub>t</sub><span style="font-family:&#39;微软雅黑&#39;,sans-serif">−</span>1+(1<span style="font-family:&#39;微软雅黑&#39;,sans-serif">−</span>β)θ<sub>t</sub></p><p>当β=0.9时，指数加权平均最后的结果如图2中红色线所示；</p><p>当β=0.98时，指数加权平均最后的结果如图2中绿色线所示；</p><p>当β=0.5时，指数加权平均最后的结果如下图2中黄色线所示。</p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518336568122890.jpg" title="1518336568122890.jpg" alt="1518336568122890.jpg" width="593" height="321"/></p><p style="text-align: center;">图2</p><p><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F;background:white">在计算当前时刻的平均值，只需要前一天的平均值和当前时刻的值，所以在数据量非常大的情况下，指数加权平均在节约计算成本的方面是一种非常有效的方式，可以很大程度上减少计算机资源存储和内存的占用。</span></p><h4 style="margin-top:8px;margin-right:0;margin-bottom:0;margin-left: 0;margin-bottom:0;line-height:normal;background:white"><strong><span style="font-size:20px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">指数加权平均的偏差修正</span></strong></h4><p style=";text-align: justify;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">在我们执行指数加权平均的公式时，当</span><span style="box-sizing: border-box;display:inline-block"><span style="font-size: 20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F"><span style="box-sizing: border-box;display:inline-block"><span style="box-sizing: border-box;outline: 0px"><span style="box-sizing: border-box"><span style="box-sizing: border-box">β</span><span style="box-sizing: border-box">=</span></span><span style="font-size: 20px;font-family: MathJax_Main, serif">0.98</span></span><span style="box-sizing: border-box;border-color:transparent !important;display:inline-block"></span></span></span></span>时，我们得到的并不是图3中的绿色曲线，而是下图中的紫色曲线，其起点比较低。</p><p>原因：</p><p>v0=0v1=0.98v0+0.02θ1=0.02θ1v2=0.98v1+0.02θ2=0.98×0.02θ1+0.02θ2=0.0196θ1+0.02θ2</p><p>如果第一天的值为如40，则得到的v1=0.02×40=8，则得到的值要远小于实际值，后面几天的情况也会由于初值引起的影响，均低于实际均值。</p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518336613102055.jpg" title="1518336613102055.jpg" alt="1518336613102055.jpg" width="650" height="319"/></p><p style="text-align: center;">图3</p><p>为了解决这个问题，我们可以做以下修正：</p><p><span style=";font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F;background:white">使用</span><span style=";font-family: &#39;Times New Roman&#39;,serif;color:#4F4F4F;background: white"><span style="box-sizing: border-box;display:inline-block"><span style="box-sizing: border-box"><span style="box-sizing: border-box"></span></span></span></span><span style="font-size:14px;font-family:&#39;Times New Roman&#39;,serif;color:#4F4F4F;background:white">v</span><span style="box-sizing: border-box;border-color: transparent !important">t</span><span style="box-sizing: border-box;clip:rect(1.249em 1000em 2.659em -0.377em)"><span style="box-sizing: border-box"><span style="box-sizing: border-box">/(</span></span><span style="font-size:14px;font-family:&#39;Times New Roman&#39;,serif;color:#4F4F4F;background:white">1</span><span style="font-size: 14px;font-family:&#39;Times New Roman&#39;,serif;color:#4F4F4F;background:white">−</span><span style="box-sizing: border-box;border-color:transparent !important;display:inline-block"><span style="font-size:14px;font-family:&#39;Times New Roman&#39;,serif;color:#4F4F4F;background:white"><span style="box-sizing: border-box;clip:rect(1.412em 1000em 2.659em -0.485em)"><span style="box-sizing: border-box">β<span style="box-sizing: border-box;border-color:transparent !important;display:inline-block;overflow:hidden"></span></span></span><span style="box-sizing: border-box;border-color:transparent !important"><span style="box-sizing: border-box"><span style="box-sizing: border-box;border-color:transparent !important"><span style="box-sizing: border-box;border-color:transparent !important">t</span></span></span></span></span></span></span>)</p><p><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#454545;background:white">在使用偏差修正后我们得到了绿色的曲线，在初始阶段，由于有了偏差修正，所以能够得到比原来图像更好的结果。随着</span><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#454545;background:white"><span style="box-sizing: border-box;display: inline-block"><span style="box-sizing: border-box"><span style="box-sizing: border-box;outline: 0px"><span style="box-sizing: border-box;display:inline-block"><span style="box-sizing: border-box;border-color:transparent !important;clip:rect(1.466em 1000em 2.442em -0.485em)"><span style="box-sizing: border-box"><span style="box-sizing: border-box;border-color:transparent !important">t</span></span></span></span><span style="box-sizing: border-box;border-color:transparent !important;display:inline-block"></span></span></span></span></span>逐渐增大，<span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#454545;background:white"><span style="box-sizing: border-box;display: inline-block"><span style="box-sizing: border-box"><span style="box-sizing: border-box;border-color:transparent !important;display:inline-block"><span style="box-sizing: border-box;clip:rect(1.412em 1000em 2.659em -0.485em)"><span style="box-sizing: border-box">β</span></span><span style="box-sizing: border-box;border-color: transparent !important"><span style="box-sizing: border-box"><span style="box-sizing: border-box;border-color:transparent !important"><span style="box-sizing: border-box;border-color:transparent !important">t</span></span></span></span></span></span></span></span>接近于0，即分母接近于1，所以后面绿色的曲线和紫色的曲线逐渐重合了。</p><h3 style="margin-top:8px;margin-right:0;margin-bottom:0;margin-left: 0;margin-bottom:0;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">3. </span><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">动量（Momentum）梯度下降法</span></h3><p style=";text-align: justify;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">动量梯度下降的基本思想与指数加权平均类似，但是它是计算梯度的指数加权平均数，并利用该梯度来更新权重。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">我们在利用动量梯度下降法来最小化函数时，每一次迭代都是一种上下波动，如果无法做好限制，就会出现幅度比较大，这会降低梯度下降进行的速度。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">我们希望我们的梯度下降能够在纵轴方向缓慢一点，横轴上速度快一点，这里我们就可以采用动量梯度下降算法就可以实现这个目的。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">算法的过程如下所示：</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F"></span></p><p>On iteration t:</p><p>Compute dw, db on the current mini-batch</p><p>vdW= βvdW+1-βdW</p><p>vdb = βvdb + 1 -βdb</p><p>W = WavdW, b=b-avdb</p><p>这里我们用到的超参数有α和β，其中β=0.9。</p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F"><br/></span><br/></p><h3 style="margin-top:8px;margin-right:0;margin-bottom:0;margin-left: 0;margin-bottom:0;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">4. RMSprop</span></h3><p style=";text-align: justify;text-indent: 32px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">除去上面的动量梯度下降算法，还有一种叫做RMSprop算法可以加快梯度下降，这个算法类似于上面的算法。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">该算法计算过程如图4所示。</span></p><p style="margin-bottom: 0px; text-align: center; text-indent: 32px; background: white;"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F"><img src="/wp-content/uploads/image/20180211/1518336685103477.jpg" title="1518336685103477.jpg" alt="1518336685103477.jpg" width="592" height="272"/></span></p><p style="margin-bottom: 0px; text-align: center; text-indent: 32px; background: white;"><span style="color:#4f4f4f;font-family:微软雅黑, sans-serif">图4</span></p><p style="margin-bottom: 0px; text-indent: 32px; background: white;"><br/></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">可以看出来，这个算法确实类似于3中的算法，只不过是在每一步中的dw和db都分别做了平方，然后在最后减去的时候也做了处理。</span></p><h3 style="margin-top:8px;margin-right:0;margin-bottom:0;margin-left: 0;margin-bottom:0;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">5. Adam </span><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">优化算法</span></h3><p style=";text-align: justify;text-indent: 32px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Adam </span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">优化算法就是将 Momentum 和 RMSprop 结合起来形成的一种适用于不同深度学习结构的优化算法。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">其算法过程如下所示，在这里借助参考文章里面的书写来表示：</span></p><p style="margin-bottom: 0px; text-indent: 32px; background: white; text-align: center;"><span style="color:#4f4f4f;font-family:微软雅黑, sans-serif"><img src="/wp-content/uploads/image/20180211/1518336713700303.jpg" title="1518336713700303.jpg" alt="1518336713700303.jpg" width="682" height="301"/></span></p><p style="margin-bottom: 0px; text-indent: 32px; background: white; text-align: center;"><span style="color:#4f4f4f;font-family:微软雅黑, sans-serif">图5</span></p><p style="margin-bottom: 0px; text-indent: 32px; background: white;"><br/></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">这里面用到的超参数如下：</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">α：需要调试</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size: 14px;font-family: MathJax_Math-italic, serif;color: #454545">β</span><span style="box-sizing: border-box;border-color: transparent !important">1</span><span style="font-size: 14px;font-family: MathJax_Main, serif;color: #454545">&amp; </span><span style="font-size: 14px;font-family: MathJax_Main, serif;color: #454545">β<sub>2</sub></span><span style="font-size: 14px;font-family: 微软雅黑, sans-serif;color: #454545">：可以调试，但是一般不用调试，</span><span style="font-size: 14px;font-family: MathJax_Math-italic, serif;color: #454545">β</span><span style="box-sizing: border-box;border-color: transparent !important">1</span><span style="font-size: 14px;font-family: 微软雅黑, sans-serif;color: #454545">默认为</span><span style="font-size: 14px;font-family: MathJax_Main, serif;color: #454545">0.9</span><span style="font-size: 14px;font-family: 微软雅黑, sans-serif;color: #454545">，</span><span style="font-size: 14px;font-family: MathJax_Math-italic, serif;color: #454545">β</span><span style="box-sizing: border-box;border-color: transparent !important"><span style="box-sizing: border-box"><span style="box-sizing: border-box;border-color:transparent !important"><span style="box-sizing: border-box;border-color:transparent !important">2</span></span><span style="font-size: 14px;font-family: 微软雅黑, sans-serif;color: #454545">默认为</span><span style="font-size: 14px;font-family: MathJax_Main, serif;color: #454545">0.999</span></span></span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size: 14px;font-family: 微软雅黑, sans-serif;color: #454545">ε推荐值为</span><span style="font-size: 14px;font-family: MathJax_Main, serif;color: #454545">10<sup>-8</sup></span></p><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">6. </span><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">学习率衰减</span></h3><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">要实现学习率衰减，有以下几种方法：</span></p><p style="background:white"><span style="font-size:14px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">（1）α=（1/1+decay_rate*epoch_num）·α<sub>0</sub></span></p><p style="background:white"><span style="font-size:14px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">（2）α=0.95<sup>epoch_num·</sup>α<sub>0</sub></span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">（3）α=（k/epoch_num）·α<sub>0</sub></span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;background:white"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">（4）离散下降</span></p><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">7. </span><span style="font-size:22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">局部最优问题</span></h3><p style="margin: 0 0 16px;text-align: justify;text-indent: 32px;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">在低维度的情形下，我们可能会想象到一个Cost function 如左图所示，存在一些局部最小值点，在初始化参数的时候，如果初始值选取的不得当，会存在陷入局部最优点的可能性。</span></p><p style="margin: 0 0 16px;text-align: justify;text-indent: 32px;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">但是，实际上如果我们建立一个神经网络，由于维度较高，通常梯度为零的点并不是如左图中的局部最优点，而是右图中的<strong style="box-sizing: border-box">鞍点</strong>。</span></p><p style="margin-bottom: 0px; text-align: center; background: white;"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F"><img src="/wp-content/uploads/image/20180211/1518336755390158.jpg" title="1518336755390158.jpg" alt="1518336755390158.jpg" width="563" height="228"/></span></p><p style="margin-bottom: 0px; text-align: center; background: white;"><span style="font-size:14px;font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">图6</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:28px;background:white"><span style="font-size: 14px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">我们这样考虑，假设参数维度为2000。那么，若是局部最优点，则要求在该点所有参数都必须是凹函数的凹点，概率为0.5<sup>2000</sup>，所以概率非常低。故在高维度情况下，梯度为0多位鞍点。</span></p><p style=";margin-bottom:0;text-align:justify;text-justify: inter-ideograph;text-indent:28px;background:white"><span style="font-size: 14px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">在高维度情况下，若要解决这个问题，可以利用如Adam等算法进行改善。</span></p><p style="margin-bottom: 0px; text-indent: 32px; background: white;"><span style="color:#4f4f4f;font-family:微软雅黑, sans-serif"><br/></span><br/></p><p><em style="font-size: 14px; white-space: normal; box-sizing: border-box; margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: rgb(255, 255, 255); word-wrap: break-word; color: rgb(65, 65, 65); font-family: &quot;Open Sans&quot;, &quot;Helvetica Neue&quot;, Helvetica, Arial, sans-serif;"><strong style="box-sizing: border-box; margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent; word-wrap: break-word;"><span style="box-sizing: border-box; margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: baseline; background: transparent; word-wrap: break-word; color: #7F7F7F;">本笔记基于DEEPLEARNING.AI中Andrew Ng课程笔记整理而成，部分内容参考了http://blog.csdn.net/koala_tree/article/details/78199611内容，在此对其表示感谢~</span></strong></em><span style="font-size: 14px;"></span></p>