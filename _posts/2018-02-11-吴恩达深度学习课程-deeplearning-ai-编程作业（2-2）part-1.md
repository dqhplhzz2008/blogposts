---
ID: 3615
post_title: >
  吴恩达深度学习课程
  DeepLearning.ai 编程作业（2-2）
post_name: '%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b-deeplearning-ai-%e7%bc%96%e7%a8%8b%e4%bd%9c%e4%b8%9a%ef%bc%882-2%ef%bc%89part-1'
author: 小奥
post_date: 2018-02-11 11:23:41
layout: post
link: >
  http://www.yushuai.me/2018/02/11/3615.html
published: true
tags:
  - Python
  - 人工智能
  - 神经网络
categories:
  - Deep Learning
---
<p style="margin: 18px 0 9px"><span style="font-size: 33px;font-family: 宋体">优化方法</span></p><p style="text-indent: 28px;line-height: 20px"><span style="font-family: 宋体">直到现在，你已经能够经常使用梯度下降来更新你的参数和最小化代价。在本次作业中，你将学到更多优秀的优化方法以来加速学习速度，并且可能最后得到一个更好的代价函数的最终结果。拥有一个好的优化算法要比等待数日或数小时来得到一个好结果更有意义。</span></p><p style="text-indent: 28px;line-height: 20px"><span style="font-family: 宋体">梯度下降就像是一个代价函数</span><span style="font-family: Helvetica, sans-serif">J</span><span style="font-family: 宋体">下山一般。</span></p><p style="text-indent:28px"><span style="font-family: 宋体">为了能够开始本作业，首先当然是载入所用到的包。</span></p><pre class="brush:python;toolbar:false">import&nbsp;numpy&nbsp;as&nbsp;np
import&nbsp;matplotlib.pyplot&nbsp;as&nbsp;plt
import&nbsp;scipy.io
import&nbsp;math
import&nbsp;sklearn
import&nbsp;sklearn.datasets
&nbsp;
from&nbsp;opt_utils&nbsp;import&nbsp;load_params_and_grads,&nbsp;initialize_parameters
from&nbsp;opt_utils&nbsp;import&nbsp;forward_propagation,&nbsp;backward_propagation
from&nbsp;opt_utils&nbsp;import&nbsp;compute_cost,&nbsp;predict,&nbsp;load_dataset
from&nbsp;opt_utils&nbsp;import&nbsp;predict_dec,&nbsp;plot_decision_boundary,
from&nbsp;testCases&nbsp;import&nbsp;*
&nbsp;
%matplotlib&nbsp;inline
plt.rcParams[&#39;figure.figsize&#39;]&nbsp;=&nbsp;(7.0,&nbsp;4.0)&nbsp;#&nbsp;set&nbsp;default&nbsp;size&nbsp;of&nbsp;plots
plt.rcParams[&#39;image.interpolation&#39;]&nbsp;=&nbsp;&#39;nearest&#39;
plt.rcParams[&#39;image.cmap&#39;]&nbsp;=&nbsp;&#39;gray&#39;</pre><h2>1 – 梯度下降</h2><p>具体算法原理因为在此之前都已经重述了多次，所以在此不再重述。直接贴出代码。</p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;update_parameters_with_gd
def&nbsp;update_parameters_with_gd(parameters,&nbsp;grads,&nbsp;learning_rate):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Update&nbsp;parameters&nbsp;using&nbsp;one&nbsp;step&nbsp;of&nbsp;gradient&nbsp;descent
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing
your&nbsp;parameters&nbsp;to&nbsp;be&nbsp;updated:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&#39;W&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;Wl
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&#39;b&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;bl
&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your
gradients&nbsp;to&nbsp;update&nbsp;each&nbsp;parameters:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads[&#39;dW&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dWl
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads[&#39;db&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dbl
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;--&nbsp;the&nbsp;learning&nbsp;rate,&nbsp;scalar.
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--
&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your&nbsp;updated&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(parameters)&nbsp;//&nbsp;2&nbsp;#&nbsp;number&nbsp;of&nbsp;layers&nbsp;in&nbsp;the&nbsp;neural&nbsp;networks
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Update&nbsp;rule&nbsp;for&nbsp;each&nbsp;parameter
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;l&nbsp;in&nbsp;range(L):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;W&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;W&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;str(l+1)]&nbsp;-learning_rate&nbsp;*&nbsp;grads[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;b&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;b&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;str(l+1)]&nbsp;-learning_rate&nbsp;*&nbsp;grads[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
return&nbsp;parameters</pre><p>测试一下：</p><pre class="brush:python;toolbar:false">parameters,&nbsp;grads,&nbsp;learning_rate=
update_parameters_with_gd_test_case()
parameters=
update_parameters_with_gd(parameters,&nbsp;grads,&nbsp;learning_rate)
print(&quot;W1&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;W1&quot;]))
print(&quot;b1&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;b1&quot;]))
print(&quot;W2&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;W2&quot;]))
print(&quot;b2&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;b2&quot;]))</pre><p>结果如下</p><p>W1 = [[ 1.63535156 -0.62320365 -0.53718766]</p><p>&nbsp;[-1.07799357&nbsp; 0.85639907 -2.29470142]]</p><p>b1 = [[ 1.74604067]</p><p>&nbsp;[-0.75184921]]</p><p>W2 = [[ 0.32171798 -0.25467393&nbsp; 1.46902454]</p><p>&nbsp;[-2.05617317 -0.31554548 -0.3756023 ]</p><p>&nbsp;[ 1.1404819&nbsp; -1.09976462 -0.1612551 ]]</p><p>b2 = [[-0.88020257]</p><p>&nbsp;[ 0.02561572]</p><p>&nbsp;[ 0.57539477]]</p><p style="text-indent:28px">梯度下降的一种变体是随机梯度下降（SGD），这相当于小批量梯度下降(mini-batch)，其中每个小批量只有1个例子。您刚刚实现的更新规则没有改变。发生变化的是，您将只计算一个训练实例的梯度，而不是整个培训集。下面的代码示例说明了随机梯度下降和（批）梯度下降的区别。</p><p>(Batch) Gradient Descent:</p><p>X = data_input</p><p>Y = labels</p><p>parameters = initialize_parameters(layers_dims)</p><p>for i in range(0, num_iterations):</p><p>&nbsp;&nbsp;&nbsp; # Forward propagation</p><p>&nbsp;&nbsp;&nbsp; a, caches = forward_propagation(X, parameters)</p><p>&nbsp;&nbsp;&nbsp; # Compute cost.</p><p>&nbsp;&nbsp;&nbsp; cost = compute_cost(a, Y)</p><p>&nbsp;&nbsp;&nbsp; # Backward propagation.</p><p>&nbsp;&nbsp;&nbsp; grads = backward_propagation(a, caches, parameters)</p><p>&nbsp;&nbsp;&nbsp; # Update parameters.</p><p>&nbsp;&nbsp;&nbsp; parameters = update_parameters(parameters, grads)</p><p>&nbsp;</p><p>•Stochastic Gradient Descent:</p><p>X = data_input</p><p>Y = labels</p><p>parameters = initialize_parameters(layers_dims)</p><p>for i in range(0, num_iterations):</p><p>&nbsp;&nbsp;&nbsp; for j in range(0, m):</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Forward propagation</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; a, caches = forward_propagation(X[:,j], parameters)</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Compute cost</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; cost = compute_cost(a, Y[:,j])</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Backward propagation</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; grads = backward_propagation(a, caches, parameters)</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; # Update parameters.</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; parameters = update_parameters(parameters, grads)</p><p style="text-indent:28px">在SGD中，在更新所有梯度之前你只需要使用1个训练例子。当训练集非常大时，SGD速度会非常快。。但是这些参数会“振荡”到最小值而不是平滑地收敛。下面是一个例子：</p><p style="text-align: center;">&nbsp;<img src="/wp-content/uploads/image/20180211/1518360027978593.jpg" title="1518360027978593.jpg" alt="1518360027978593.jpg" width="600" height="175"/></p><p style="text-align: center;"><strong><span style="text-decoration:underline;"><span style=";font-family:&#39;Helvetica&#39;,sans-serif;color:purple">Figure 1</span></span></strong><span style="text-decoration:underline;"> </span><span style=";font-family:&#39;Helvetica&#39;,sans-serif;color:purple">: <strong>SGD vs GD</strong><br/> &quot;+&quot; denotes a minimum of the cost. SGD leads to many oscillations to reach convergence. But each step is a lot faster to compute for SGD than for GD, as it uses only one training example (vs. the whole batch for GD).</span></p><h2>2 - Mini-Batch梯度下降算法</h2><p style="line-height:20px"><span style="font-size: 14px">让我们来学习如何从训练集</span><span style="font-size: 14px;font-family: Helvetica, sans-serif"> (X, Y)</span><span style="font-size: 14px">利用</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">mini-batch</span><span style="font-size: 14px">进行训练。</span></p><p style="line-height:20px"><span style="font-size: 14px">分两步</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">:</span></p><ul class=" list-paddingleft-2"><li><p><strong><span style=";font-family:&#39;Helvetica&#39;,sans-serif">Shuffle</span></strong><span style=";font-family:&#39;Helvetica&#39;,sans-serif">: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the </span><em><span style="font-size:17px;font-family:STIXMathJax_Main-Web">i</span></em><span style="font-size:17px;font-family:&#39;Helvetica&#39;,sans-serif">&nbsp;</span><em>th</em><span style="font-size:17px;font-family:&#39;Helvetica&#39;,sans-serif">&nbsp;</span>column of X is the example corresponding to the <span style="display:inline-block"><em><span style="font-size:17px;font-family:STIXMathJax_Main-Web"><span style="display:inline-block;overflow:hidden"><span style="display:inline-block"></span><span style="display:inline-block"><span style="clip:rect(2.99em, 1000.96em, 4.21em, -1000em)"><span style="display:inline-block"><span style="clip:rect(3.18em, 1000.26em, 4.21em, -1000em)">i</span></span></span></span></span></span></em></span><em><span style="font-size:12px;font-family:STIXMathJax_Main-Web">t</span>h</em><span style="font-size:17px;font-family:&#39;Helvetica&#39;,sans-serif">&nbsp;</span>label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches.</p></li></ul><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360080398769.jpg" title="1518360080398769.jpg" alt="1518360080398769.jpg" width="546" height="291"/></p><p><strong><span style="font-family: Helvetica, sans-serif">Partition</span></strong><span style="font-family: Helvetica, sans-serif">: Partition the shuffled (X, Y) into mini-batches of size </span><code><span style="font-size:13px">mini_batch_size</span></code><span style="font-family: Helvetica, sans-serif"> (here 64). Note that the number of training examples is not always divisible by </span><code><span style="font-size:13px">mini_batch_size</span></code><span style="font-family: Helvetica, sans-serif">. The last mini batch might be smaller, but you don&#39;t need to worry about this. When the final mini-batch is smaller than the full </span><code><span style="font-size:13px">mini_batch_size</span></code><span style="font-family: Helvetica, sans-serif">, it will look like this:</span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360193455856.jpg" title="1518360193455856.jpg" alt="1518360193455856.jpg" width="550" height="308"/></p><p style="line-height: 20px"><strong><span style="font-family: Helvetica, sans-serif">Exercise</span></strong><span style="font-family: Helvetica, sans-serif">: Implement </span><span style="font-size: 13px;font-family: &#39;Courier New&#39;;background: #F9F2F4">random_mini_batches</span><span style="font-family: Helvetica, sans-serif">. We coded the shuffling part for you. To help you with the partitioning step, we give you the following code that selects the indexes for the </span><span style="font-size: 17px;font-family: STIXMathJax_Main-Web;border: 1px none windowtext;padding: 0">1</span><em><span style="font-size: 12px;font-family: STIXMathJax_Main-Web;border: 1px none windowtext;padding: 0">st</span></em><span style="font-size: 17px;font-family: Helvetica, sans-serif;border: 1px none windowtext;padding: 0">&nbsp;</span><span style="font-family: Helvetica, sans-serif">and </span><span style="font-size: 17px;font-family: STIXMathJax_Main-Web;border: 1px none windowtext;padding: 0">2</span><span style="font-size: 17px;font-family: Helvetica, sans-serif;border: 1px none windowtext;padding: 0">&nbsp;</span><em><span style="font-size: 12px;font-family: STIXMathJax_Main-Web;border: 1px none windowtext;padding: 0">nd</span></em><span style="font-size: 17px;font-family: Helvetica, sans-serif;border: 1px none windowtext;padding: 0">&nbsp;</span><span style="font-family: Helvetica, sans-serif">mini-batches:</span></p><p>first_mini_batch_X = shuffled_X[:, 0 : mini_batch_size]</p><p>second_mini_batch_X = shuffled_X[:, mini_batch_size : 2 * mini_batch_size]</p><p>...</p><p><br/></p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;random_mini_batches
def&nbsp;random_mini_batches(X,&nbsp;Y,
mini_batch_size=64,seed&nbsp;=&nbsp;0):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Creates&nbsp;a&nbsp;list&nbsp;of&nbsp;random&nbsp;minibatches&nbsp;from&nbsp;(X,&nbsp;Y)
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;--
input&nbsp;data,&nbsp;of&nbsp;shape&nbsp;(input&nbsp;size,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;Y&nbsp;--
&nbsp;true&nbsp;&quot;label&quot;&nbsp;vector&nbsp;(1&nbsp;for&nbsp;blue&nbsp;dot&nbsp;/&nbsp;0&nbsp;for&nbsp;red&nbsp;dot),
&nbsp;of&nbsp;shape&nbsp;(1,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;mini_batch_size&nbsp;--&nbsp;size&nbsp;of&nbsp;the&nbsp;mini-batches,&nbsp;integer
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;mini_batches&nbsp;--&nbsp;list&nbsp;of&nbsp;synchronous
&nbsp;(mini_batch_X,&nbsp;mini_batch_Y)
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;#&nbsp;To&nbsp;make&nbsp;your&nbsp;&quot;random&quot;&nbsp;minibatches&nbsp;the&nbsp;same&nbsp;as&nbsp;ours&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;np.random.seed(seed)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;m&nbsp;=&nbsp;X.shape[1]&nbsp;&nbsp;&nbsp;#&nbsp;number&nbsp;of&nbsp;training&nbsp;examples
&nbsp;&nbsp;&nbsp;&nbsp;mini_batches&nbsp;=&nbsp;[]
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Step&nbsp;1:&nbsp;Shuffle&nbsp;(X,&nbsp;Y)
&nbsp;&nbsp;&nbsp;&nbsp;permutation&nbsp;=&nbsp;list(np.random.permutation(m))
&nbsp;&nbsp;&nbsp;&nbsp;shuffled_X&nbsp;=&nbsp;X[:,&nbsp;permutation]&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;shuffled_Y&nbsp;=&nbsp;Y[:,&nbsp;permutation].reshape((1,m))
#&nbsp;Step&nbsp;2:&nbsp;Partition&nbsp;(shuffled_X,&nbsp;shuffled_Y).Minus&nbsp;the&nbsp;end&nbsp;case.
&nbsp;&nbsp;&nbsp;&nbsp;num_complete_minibatches&nbsp;=&nbsp;math.floor(m/mini_batch_size)
&nbsp;#&nbsp;number&nbsp;of&nbsp;mini&nbsp;batches&nbsp;of&nbsp;size&nbsp;mini_batch_size&nbsp;in&nbsp;your&nbsp;partitionning
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;k&nbsp;in&nbsp;range(0,&nbsp;num_complete_minibatches):
###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch_X&nbsp;=&nbsp;shuffled_X[:,&nbsp;k&nbsp;*&nbsp;mini_batch_size&nbsp;:
(k+1)&nbsp;*&nbsp;mini_batch_size]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch_Y&nbsp;=&nbsp;shuffled_Y[:,&nbsp;k&nbsp;*&nbsp;mini_batch_size&nbsp;:
(k+1)&nbsp;*&nbsp;mini_batch_size]
###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch&nbsp;=&nbsp;(mini_batch_X,&nbsp;mini_batch_Y)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batches.append(mini_batch)
#&nbsp;Handling&nbsp;the&nbsp;end&nbsp;case&nbsp;(last&nbsp;mini-batch&nbsp;&lt;&nbsp;mini_batch_size)
&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;m&nbsp;%&nbsp;mini_batch_size&nbsp;!=&nbsp;0:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch_X&nbsp;=&nbsp;shuffled_X[:,&nbsp;mini_batch_size&nbsp;*
num_complete_minibatches&nbsp;:&nbsp;m]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch_Y&nbsp;=&nbsp;shuffled_Y[:,&nbsp;mini_batch_size&nbsp;*
num_complete_minibatches&nbsp;:&nbsp;m]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batch&nbsp;=&nbsp;(mini_batch_X,&nbsp;mini_batch_Y)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mini_batches.append(mini_batch)
&nbsp;&nbsp;&nbsp;
return&nbsp;mini_batches
测试一下：X_assess,&nbsp;Y_assess,&nbsp;mini_batch_size&nbsp;=
random_mini_batches_test_case()
mini_batches&nbsp;=&nbsp;random_mini_batches
(X_assess,&nbsp;Y_assess,&nbsp;mini_batch_size)
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;1st&nbsp;mini_batch_X:&nbsp;&quot;
+&nbsp;str(mini_batches[0][0].shape))
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;2nd&nbsp;mini_batch_X:&nbsp;&quot;
+&nbsp;str(mini_batches[1][0].shape))
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;3rd&nbsp;mini_batch_X:&nbsp;&quot;
+&nbsp;str(mini_batches[2][0].shape))
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;1st&nbsp;mini_batch_Y:&nbsp;&quot;
&nbsp;+&nbsp;str(mini_batches[0][1].shape))
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;2nd&nbsp;mini_batch_Y:&nbsp;&quot;
+&nbsp;str(mini_batches[1][1].shape))
print&nbsp;(&quot;shape&nbsp;of&nbsp;the&nbsp;3rd&nbsp;mini_batch_Y:&nbsp;&quot;
+&nbsp;str(mini_batches[2][1].shape))
print&nbsp;(&quot;mini&nbsp;batch&nbsp;sanity&nbsp;check:&nbsp;&quot;&nbsp;+
str(mini_batches[0][0][0][0:3]))</pre><p>结果：</p><p>shape of the 1st mini_batch_X: (12288, 64)</p><p>shape of the 2nd mini_batch_X: (12288, 64)</p><p>shape of the 3rd mini_batch_X: (12288, 20)</p><p>shape of the 1st mini_batch_Y: (1, 64)</p><p>shape of the 2nd mini_batch_Y: (1, 64)</p><p>shape of the 3rd mini_batch_Y: (1, 20)</p><p>mini batch sanity check: [ 0.90085595 -0.7612069&nbsp;&nbsp; 0.2344157 ]</p><h2>3 – 动量梯度下降算法</h2><p>Exercise: 初始化向量v。向量v是一个python中的字典，初始化为一组0。这一步的关键跟在梯度向量中类似。</p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;initialize_velocity
def&nbsp;initialize_velocity(parameters):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Initializes&nbsp;the&nbsp;velocity&nbsp;as&nbsp;a&nbsp;python&nbsp;dictionary&nbsp;with:
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;keys:&nbsp;&quot;dW1&quot;,&nbsp;&quot;db1&quot;,&nbsp;...,&nbsp;&quot;dWL&quot;,&nbsp;&quot;dbL&quot;
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;values:&nbsp;numpy&nbsp;arrays&nbsp;of&nbsp;zeros&nbsp;of&nbsp;the&nbsp;same&nbsp;shape
as&nbsp;the&nbsp;corresponding&nbsp;gradients/parameters.
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your&nbsp;parameters.
&nbsp;parameters[&#39;W&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;Wl
&nbsp;parameters[&#39;b&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;bl
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;--
python&nbsp;dictionary&nbsp;containing&nbsp;the&nbsp;current&nbsp;velocity.
v[&#39;dW&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;velocity&nbsp;of&nbsp;dWl
&nbsp;v[&#39;db&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;velocity&nbsp;of&nbsp;dbl
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(parameters)&nbsp;//&nbsp;2
&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;=&nbsp;{}
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;velocity
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;l&nbsp;in&nbsp;range(L):
&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;W&quot;
+&nbsp;str(l+1)].shape)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;b&quot;
&nbsp;+&nbsp;str(l+1)].shape)
&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;&nbsp;&nbsp;
return&nbsp;v</pre><p>测试一下：</p><pre class="brush:python;toolbar:false">parameters&nbsp;=&nbsp;initialize_velocity_test_case()
&nbsp;
v&nbsp;=&nbsp;initialize_velocity(parameters)
print(&quot;v[\&quot;dW1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW1&quot;]))
print(&quot;v[\&quot;db1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db1&quot;]))
print(&quot;v[\&quot;dW2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW2&quot;]))
print(&quot;v[\&quot;db2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db2&quot;]))</pre><p><strong><span style="font-family: Helvetica, sans-serif">Exercise</span></strong><span style="font-family: Helvetica, sans-serif">: Now, implement the parameters update with momentum. The momentum update rule is, for </span><span style="display:inline-block"><em><span style="font-size: 17px;font-family: STIXMathJax_Main-Web"><span style="display:inline-block;overflow:hidden"><span style="display:inline-block"></span><span style="display:inline-block"><span style="clip:rect(3.15em, 1004.8em, 4.34em, -1000em)">l</span></span></span></span></em></span>=1,...,<em><span style="font-size: 17px;font-family: STIXMathJax_Main-Web">L</span></em>&nbsp;l=1,...,L<span style="font-family: Helvetica, sans-serif">:</span></p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:
#update_parameters_with_momentum
def&nbsp;update_parameters_with_momentum
(parameters,&nbsp;grads,&nbsp;v,&nbsp;beta,&nbsp;learning_rate):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Update&nbsp;parameters&nbsp;using&nbsp;Momentum
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--
python&nbsp;dictionary&nbsp;containing&nbsp;your&nbsp;parameters:
parameters[&#39;W&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;Wl
parameters[&#39;b&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;bl
grads&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your
&nbsp;gradients&nbsp;for&nbsp;each&nbsp;parameters:
grads[&#39;dW&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dWl
grads[&#39;db&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dbl
v&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;the
current&nbsp;velocity:
v[&#39;dW&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
v[&#39;db&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
beta&nbsp;--&nbsp;the&nbsp;momentum&nbsp;hyperparameter,&nbsp;scalar
learning_rate&nbsp;--&nbsp;the&nbsp;learning&nbsp;rate,&nbsp;scalar
Returns:
parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your
updated&nbsp;parameters
v&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your&nbsp;updated
&nbsp;velocities
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(parameters)&nbsp;//&nbsp;2
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Momentum&nbsp;update&nbsp;for&nbsp;each&nbsp;parameter
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;l&nbsp;in&nbsp;range(L):
###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;4&nbsp;lines)
&nbsp;#&nbsp;compute&nbsp;velocities
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta&nbsp;*&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]
+&nbsp;(1.&nbsp;-&nbsp;beta)&nbsp;*&nbsp;grads[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta&nbsp;*&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
+&nbsp;(1.&nbsp;-&nbsp;beta)&nbsp;*&nbsp;grads[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
#&nbsp;update&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;W&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;W&quot;
&nbsp;+&nbsp;str(l+1)]-&nbsp;learning_rate&nbsp;*&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;b&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;b&quot;
+&nbsp;str(l+1)]-&nbsp;learning_rate&nbsp;*&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
return&nbsp;parameters,&nbsp;v</pre><p>测试一下：</p><p><span style="font-family: &#39;Courier New&#39;"></span></p><pre class="brush:python;toolbar:false">parameters,&nbsp;grads,&nbsp;v&nbsp;=&nbsp;
update_parameters_with_momentum_test_case()
&nbsp;
parameters,&nbsp;v&nbsp;=&nbsp;update_parameters_
with_momentum(parameters,&nbsp;grads,&nbsp;v,&nbsp;
beta&nbsp;=&nbsp;0.9,&nbsp;learning_rate&nbsp;=&nbsp;0.01)
print(&quot;W1&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;W1&quot;]))
print(&quot;b1&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;b1&quot;]))
print(&quot;W2&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;W2&quot;]))
print(&quot;b2&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(parameters[&quot;b2&quot;]))
print(&quot;v[\&quot;dW1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW1&quot;]))
print(&quot;v[\&quot;db1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db1&quot;]))
print(&quot;v[\&quot;dW2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW2&quot;]))
print(&quot;v[\&quot;db2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db2&quot;]))</pre><p><span style="font-size: 14px;font-family: &#39;Courier New&#39;"></span></p><h2>4 – Adam算法</h2><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum</span></p><p>&nbsp;</p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;initialize_adam
def&nbsp;initialize_adam(parameters)&nbsp;:
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Initializes&nbsp;v&nbsp;and&nbsp;s&nbsp;as&nbsp;two&nbsp;python&nbsp;dictionaries&nbsp;with:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;keys:&nbsp;&quot;dW1&quot;,&nbsp;&quot;db1&quot;,&nbsp;...,&nbsp;&quot;dWL&quot;,&nbsp;&quot;dbL&quot;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;values:&nbsp;numpy&nbsp;arrays&nbsp;of&nbsp;zeros&nbsp;of&nbsp;the&nbsp;same
shape&nbsp;as&nbsp;the&nbsp;corresponding&nbsp;gradients/parameters.
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your
parameters.
&nbsp;parameters[&quot;W&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;Wl
&nbsp;parameters[&quot;b&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;bl
&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;that&nbsp;will&nbsp;contain&nbsp;the&nbsp;exponentially
&nbsp;weighted&nbsp;average&nbsp;of&nbsp;the&nbsp;gradient.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;that&nbsp;will&nbsp;contain&nbsp;the&nbsp;exponentially
weighted&nbsp;average&nbsp;of&nbsp;the&nbsp;squared&nbsp;gradient.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;dW&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;db&quot;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;...
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(parameters)&nbsp;//&nbsp;2
&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;=&nbsp;{}
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;{}
#&nbsp;Initialize&nbsp;v,&nbsp;s.&nbsp;Input:&nbsp;&quot;parameters&quot;.&nbsp;Outputs:&nbsp;&quot;v,&nbsp;s&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;l&nbsp;in&nbsp;range(L):
&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;4&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;W&quot;&nbsp;+
&nbsp;str(l+1)].shape)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;b&quot;&nbsp;+
&nbsp;str(l+1)].shape)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;W&quot;&nbsp;+
&nbsp;str(l+1)].shape)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;np.zeros(parameters[&quot;b&quot;&nbsp;+
&nbsp;str(l+1)].shape)
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
return&nbsp;v,&nbsp;s</pre><p>测试一下：</p><pre class="brush:python;toolbar:false">parameters&nbsp;=&nbsp;initialize_adam_test_case()
&nbsp;
v,&nbsp;s&nbsp;=&nbsp;initialize_adam(parameters)
print(&quot;v[\&quot;dW1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW1&quot;]))
print(&quot;v[\&quot;db1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db1&quot;]))
print(&quot;v[\&quot;dW2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;dW2&quot;]))
print(&quot;v[\&quot;db2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(v[&quot;db2&quot;]))
print(&quot;s[\&quot;dW1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(s[&quot;dW1&quot;]))
print(&quot;s[\&quot;db1\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(s[&quot;db1&quot;]))
print(&quot;s[\&quot;dW2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(s[&quot;dW2&quot;]))
print(&quot;s[\&quot;db2\&quot;]&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(s[&quot;db2&quot;]))</pre><p><strong><span style="font-family: Helvetica, sans-serif">Exercise</span></strong><span style="font-family: Helvetica, sans-serif">: Now, implement the parameters update with Adam</span></p><p><span style="font-family: Helvetica, sans-serif"></span></p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;
update_parameters_with_adam
def&nbsp;update_parameters_with_adam
(parameters,&nbsp;grads,&nbsp;v,&nbsp;s,&nbsp;t,&nbsp;learning_rate&nbsp;=&nbsp;0.01,
beta1&nbsp;=&nbsp;0.9,&nbsp;beta2&nbsp;=&nbsp;0.999,&nbsp;&nbsp;epsilon&nbsp;=&nbsp;1e-8):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Update&nbsp;parameters&nbsp;using&nbsp;Adam
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;
your&nbsp;parameters:
parameters[&#39;W&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;Wl
parameters[&#39;b&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;bl
&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;your&nbsp;
gradients&nbsp;for&nbsp;each&nbsp;parameters:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads[&#39;dW&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dWl
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads[&#39;db&#39;&nbsp;+&nbsp;str(l)]&nbsp;=&nbsp;dbl
v&nbsp;--&nbsp;Adam&nbsp;variable,&nbsp;moving&nbsp;average&nbsp;of&nbsp;
the&nbsp;first&nbsp;gradient,&nbsp;python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;--&nbsp;Adam&nbsp;variable,&nbsp;moving&nbsp;average&nbsp;of&nbsp;
the&nbsp;squared&nbsp;gradient,&nbsp;python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;--&nbsp;the&nbsp;learning&nbsp;rate,&nbsp;scalar.
&nbsp;&nbsp;&nbsp;&nbsp;beta1&nbsp;--&nbsp;Exponential&nbsp;decay&nbsp;hyperparameter&nbsp;
for&nbsp;the&nbsp;first&nbsp;moment&nbsp;estimates&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;beta2&nbsp;--&nbsp;Exponential&nbsp;decay&nbsp;hyperparameter&nbsp;
for&nbsp;the&nbsp;second&nbsp;moment&nbsp;estimates&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;--&nbsp;hyperparameter&nbsp;preventing&nbsp;division
&nbsp;by&nbsp;zero&nbsp;in&nbsp;Adam&nbsp;updates
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;
your&nbsp;updated&nbsp;parameters&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;--&nbsp;Adam&nbsp;variable,&nbsp;moving&nbsp;average&nbsp;of&nbsp;the&nbsp;
first&nbsp;gradient,&nbsp;python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;--&nbsp;Adam&nbsp;variable,&nbsp;moving&nbsp;average&nbsp;of&nbsp;the&nbsp;
squared&nbsp;gradient,&nbsp;python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(parameters)&nbsp;//&nbsp;2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;v_corrected&nbsp;=&nbsp;{}&nbsp;#&nbsp;Initializing&nbsp;first&nbsp;moment&nbsp;estimate,&nbsp;
python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;s_corrected&nbsp;=&nbsp;{}#&nbsp;Initializing&nbsp;second&nbsp;moment&nbsp;estimate,&nbsp;
python&nbsp;dictionary
&nbsp;&nbsp;&nbsp;&nbsp;
#&nbsp;Perform&nbsp;Adam&nbsp;update&nbsp;on&nbsp;all&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;l&nbsp;in&nbsp;range(L):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Moving&nbsp;average&nbsp;of&nbsp;the&nbsp;gradients.&nbsp;
#Inputs:&nbsp;&quot;v,&nbsp;grads,&nbsp;beta1&quot;.&nbsp;Output:&nbsp;&quot;v&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta1&nbsp;*&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
+&nbsp;(1.&nbsp;-&nbsp;beta1)&nbsp;*&nbsp;grads[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta1&nbsp;*&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
+&nbsp;(1.&nbsp;-&nbsp;beta1)&nbsp;*&nbsp;grads[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Compute&nbsp;bias-corrected&nbsp;first&nbsp;moment&nbsp;estimate
#&nbsp;Inputs:&nbsp;&quot;v,&nbsp;beta1,&nbsp;t&quot;.&nbsp;Output:&nbsp;&quot;v_corrected&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v_corrected[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;v[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
/&nbsp;(1&nbsp;-&nbsp;beta1**t)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v_corrected[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;v[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]
&nbsp;/&nbsp;(1&nbsp;-&nbsp;beta1**t)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Moving&nbsp;average&nbsp;of&nbsp;the&nbsp;squared&nbsp;gradients.&nbsp;
#Inputs:&nbsp;&quot;s,&nbsp;grads,&nbsp;beta2&quot;.&nbsp;Output:&nbsp;&quot;s&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta2&nbsp;*&nbsp;s[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
+&nbsp;(1.&nbsp;-&nbsp;beta2)&nbsp;*&nbsp;grads[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]**2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;beta2&nbsp;*&nbsp;s[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
+&nbsp;(1.&nbsp;-&nbsp;beta2)&nbsp;*&nbsp;grads[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]**2
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Compute&nbsp;bias-corrected&nbsp;second&nbsp;raw&nbsp;moment&nbsp;
#estimate.&nbsp;Inputs:&nbsp;&quot;s,&nbsp;beta2,&nbsp;t&quot;.&nbsp;Output:&nbsp;&quot;s_corrected&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s_corrected[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;s[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
/&nbsp;(1&nbsp;-&nbsp;beta2**t)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;s_corrected[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;s[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
/&nbsp;(1&nbsp;-&nbsp;beta2**t)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Update&nbsp;parameters.&nbsp;Inputs:&nbsp;&quot;parameters,
#&nbsp;learning_rate,&nbsp;v_corrected,&nbsp;s_corrected,&nbsp;epsilon&quot;.&nbsp;
#Output:&nbsp;&quot;parameters&quot;.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(approx.&nbsp;2&nbsp;lines)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;W&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;W&quot;&nbsp;+&nbsp;
str(l+1)]&nbsp;-&nbsp;learning_rate&nbsp;*&nbsp;v_corrected[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;/&nbsp;\
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(np.sqrt(s_corrected[&quot;dW&quot;&nbsp;+&nbsp;str(l+1)])&nbsp;+&nbsp;epsilon)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters[&quot;b&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;=&nbsp;parameters[&quot;b&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;
-&nbsp;learning_rate&nbsp;*&nbsp;v_corrected[&quot;db&quot;&nbsp;+&nbsp;str(l+1)]&nbsp;/&nbsp;\
&nbsp;(np.sqrt(s_corrected[&quot;db&quot;&nbsp;+&nbsp;str(l+1)])&nbsp;+&nbsp;epsilon)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
return&nbsp;parameters,&nbsp;v,&nbsp;s</pre><h2>5 – 使用不同优化算法的模型</h2><p style="line-height:20px"><span style="font-size: 14px">让我们利用“</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">moons</span><span style="font-size: 14px">”数据库来测试不同的优化模型。</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span></p><pre class="brush:python;toolbar:false">def&nbsp;model(X,&nbsp;Y,&nbsp;layers_dims,&nbsp;optimizer,&nbsp;
learning_rate&nbsp;=&nbsp;0.0007,&nbsp;mini_batch_size&nbsp;
=&nbsp;64,&nbsp;beta&nbsp;=&nbsp;0.9,&nbsp;beta1&nbsp;=&nbsp;0.9,&nbsp;beta2&nbsp;=&nbsp;
0.999,&nbsp;&nbsp;epsilon&nbsp;=&nbsp;1e-8,&nbsp;num_epochs&nbsp;=&nbsp;
10000,&nbsp;print_cost&nbsp;=&nbsp;True):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;--&nbsp;input&nbsp;data,&nbsp;of&nbsp;shape&nbsp;(2,&nbsp;number&nbsp;
of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;Y&nbsp;--&nbsp;true&nbsp;&quot;label&quot;&nbsp;vector&nbsp;(1&nbsp;for&nbsp;blue&nbsp;dot&nbsp;/&nbsp;
0&nbsp;for&nbsp;red&nbsp;dot),&nbsp;of&nbsp;shape&nbsp;(1,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;layers_dims&nbsp;--&nbsp;python&nbsp;list,&nbsp;containing
&nbsp;the&nbsp;size&nbsp;of&nbsp;each&nbsp;layer
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;--&nbsp;the&nbsp;learning&nbsp;rate,&nbsp;scalar.
&nbsp;&nbsp;&nbsp;&nbsp;mini_batch_size&nbsp;--&nbsp;the&nbsp;size&nbsp;of&nbsp;a&nbsp;mini&nbsp;batch
&nbsp;&nbsp;&nbsp;&nbsp;beta&nbsp;--&nbsp;Momentum&nbsp;hyperparameter
&nbsp;&nbsp;&nbsp;&nbsp;beta1&nbsp;--&nbsp;Exponential&nbsp;decay&nbsp;hyperparameter&nbsp;
for&nbsp;the&nbsp;past&nbsp;gradients&nbsp;estimates&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;beta2&nbsp;--&nbsp;Exponential&nbsp;decay&nbsp;hyperparameter
&nbsp;for&nbsp;the&nbsp;past&nbsp;squared&nbsp;gradients&nbsp;estimates&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;epsilon&nbsp;--&nbsp;hyperparameter&nbsp;preventing&nbsp;division
&nbsp;by&nbsp;zero&nbsp;in&nbsp;Adam&nbsp;updates
&nbsp;&nbsp;&nbsp;&nbsp;num_epochs&nbsp;--&nbsp;number&nbsp;of&nbsp;epochs
&nbsp;&nbsp;&nbsp;&nbsp;print_cost&nbsp;--&nbsp;True&nbsp;to&nbsp;print&nbsp;the&nbsp;cost&nbsp;every
&nbsp;1000&nbsp;epoch
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
parameters&nbsp;--&nbsp;python&nbsp;dictionary&nbsp;containing&nbsp;
your&nbsp;updated&nbsp;parameters&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;L&nbsp;=&nbsp;len(layers_dims)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;costs&nbsp;=&nbsp;[]&nbsp;#&nbsp;to&nbsp;keep&nbsp;track&nbsp;of&nbsp;the&nbsp;cost
&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;=&nbsp;0#&nbsp;initializing&nbsp;the&nbsp;counter&nbsp;required&nbsp;for&nbsp;Adam&nbsp;update
#&nbsp;For&nbsp;grading&nbsp;purposes,&nbsp;so&nbsp;that&nbsp;your&nbsp;&quot;random&quot;
#&nbsp;minibatches&nbsp;are&nbsp;the&nbsp;same&nbsp;as&nbsp;ours
&nbsp;&nbsp;&nbsp;&nbsp;seed&nbsp;=&nbsp;10&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;=&nbsp;initialize_parameters(layers_dims)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Initialize&nbsp;the&nbsp;optimizer
&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;optimizer&nbsp;==&nbsp;&quot;gd&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass&nbsp;#&nbsp;no&nbsp;initialization&nbsp;required&nbsp;for&nbsp;gradient&nbsp;descent
&nbsp;&nbsp;&nbsp;&nbsp;elif&nbsp;optimizer&nbsp;==&nbsp;&quot;momentum&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v&nbsp;=&nbsp;initialize_velocity(parameters)
&nbsp;&nbsp;&nbsp;&nbsp;elif&nbsp;optimizer&nbsp;==&nbsp;&quot;adam&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;v,&nbsp;s&nbsp;=&nbsp;initialize_adam(parameters)
&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Optimization&nbsp;loop
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(num_epochs):
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Define&nbsp;the&nbsp;random&nbsp;minibatches.&nbsp;We&nbsp;increment&nbsp;the&nbsp;seed&nbsp;
#to&nbsp;reshuffle&nbsp;differently&nbsp;the&nbsp;dataset&nbsp;after&nbsp;each&nbsp;epoch
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;seed&nbsp;=&nbsp;seed&nbsp;+&nbsp;1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;minibatches&nbsp;=&nbsp;random_mini_batches(X,&nbsp;Y,&nbsp;
mini_batch_size,&nbsp;seed)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;minibatch&nbsp;in&nbsp;minibatches:
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Select&nbsp;a&nbsp;minibatch
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(minibatch_X,&nbsp;minibatch_Y)&nbsp;=&nbsp;minibatch
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Forward&nbsp;propagation
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a3,&nbsp;caches&nbsp;=&nbsp;forward_propagation(minibatch_X,&nbsp;
parameters)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Compute&nbsp;cost
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#print(&quot;mini&nbsp;Batch&nbsp;Y&nbsp;shape&quot;,&nbsp;minibatch_Y.shape)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;=&nbsp;compute_cost(a3,&nbsp;minibatch_Y)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Backward&nbsp;propagation
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;=&nbsp;backward_propagation(minibatch_X,&nbsp;
minibatch_Y,&nbsp;caches)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Update&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;optimizer&nbsp;==&nbsp;&quot;gd&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters&nbsp;=&nbsp;update_parameters_with_gd
(parameters,&nbsp;grads,&nbsp;learning_rate)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif&nbsp;optimizer&nbsp;==&nbsp;&quot;momentum&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters,&nbsp;v&nbsp;=&nbsp;update_parameters_with_momentum
(parameters,&nbsp;grads,&nbsp;v,&nbsp;beta,&nbsp;learning_rate)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;elif&nbsp;optimizer&nbsp;==&nbsp;&quot;adam&quot;:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;t&nbsp;=&nbsp;t&nbsp;+&nbsp;1&nbsp;#&nbsp;Adam&nbsp;counter
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;parameters,&nbsp;v,&nbsp;s&nbsp;=&nbsp;update_parameters_with_adam
(parameters,&nbsp;grads,&nbsp;v,&nbsp;s,&nbsp;t,&nbsp;learning_rate,&nbsp;beta1,&nbsp;beta2,&nbsp;&nbsp;epsilon)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Print&nbsp;the&nbsp;cost&nbsp;every&nbsp;1000&nbsp;epoch
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;print_cost&nbsp;and&nbsp;i&nbsp;%&nbsp;1000&nbsp;==&nbsp;0:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;(&quot;Cost&nbsp;after&nbsp;epoch&nbsp;%i:&nbsp;%f&quot;&nbsp;%(i,&nbsp;cost))
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;print_cost&nbsp;and&nbsp;i&nbsp;%&nbsp;100&nbsp;==&nbsp;0:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;costs.append(cost)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;plot&nbsp;the&nbsp;cost
&nbsp;&nbsp;&nbsp;&nbsp;plt.plot(costs)
&nbsp;&nbsp;&nbsp;&nbsp;plt.ylabel(&#39;cost&#39;)
&nbsp;&nbsp;&nbsp;&nbsp;plt.xlabel(&#39;epochs&nbsp;(per&nbsp;100)&#39;)
&nbsp;&nbsp;&nbsp;&nbsp;plt.title(&quot;Learning&nbsp;rate&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(learning_rate))
&nbsp;&nbsp;&nbsp;&nbsp;plt.show()
&nbsp;
return&nbsp;parameters</pre><p style="text-indent:28px;line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span><br/></p><p style="line-height:20px"><span style="font-size: 14px">现在使用这个</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">3</span><span style="font-size: 14px">层神经网络来测试</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">3</span><span style="font-size: 14px">种优化算法。</span></p><h3>5.1 - Mini-batch Gradient descent</h3><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Run the following code to see how the model does with mini-batch gradient descent</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span></p><pre class="brush:python;toolbar:false">#&nbsp;train&nbsp;3-layer&nbsp;model
layers_dims&nbsp;=&nbsp;[train_X.shape[0],&nbsp;5,&nbsp;2,&nbsp;1]
parameters&nbsp;=&nbsp;model(train_X,&nbsp;train_Y,&nbsp;
layers_dims,&nbsp;optimizer&nbsp;=&nbsp;&quot;gd&quot;)
&nbsp;
#&nbsp;Predict
predictions&nbsp;=&nbsp;predict(train_X,&nbsp;train_Y,&nbsp;
parameters)
&nbsp;
#&nbsp;Plot&nbsp;decision&nbsp;boundary
plt.title(&quot;Model&nbsp;with&nbsp;Gradient&nbsp;Descent&nbsp;
optimization&quot;)
axes&nbsp;=&nbsp;plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda&nbsp;x:
&nbsp;predict_dec(parameters,&nbsp;x.T),&nbsp;train_X,&nbsp;train_Y)</pre><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span><br/></p><p style="line-height:20px"><span style="font-size: 14px">结果：</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 0: 0.690736</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 1000: 0.685273</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 2000: 0.647072</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 3000: 0.619525</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 4000: 0.576584</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 5000: 0.607243</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 6000: 0.529403</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 7000: 0.460768</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 8000: 0.465586</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 9000: 0.464518</span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360435168889.jpg" title="1518360435168889.jpg" alt="4.jpg"/></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360449697447.jpg" title="1518360449697447.jpg" alt="5.jpg"/></p><h3>5.2 - Mini-batch gradient descent with momentum</h3><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Run the following code to see how the model does with momentum. Because this example is relatively simple, the gains from using momemtum are small; but for more complex problems you might see bigger gains.</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span></p><pre class="brush:python;toolbar:false">#&nbsp;train&nbsp;3-layer&nbsp;model
layers_dims&nbsp;=&nbsp;[train_X.shape[0],&nbsp;5,&nbsp;2,&nbsp;1]
parameters&nbsp;=&nbsp;model(train_X,&nbsp;train_Y,&nbsp;
layers_dims,&nbsp;beta&nbsp;=&nbsp;0.9,&nbsp;optimizer&nbsp;=&nbsp;&quot;momentum&quot;)
&nbsp;
#&nbsp;Predict
predictions&nbsp;=&nbsp;predict(train_X,&nbsp;train_Y,
&nbsp;parameters)
&nbsp;
#&nbsp;Plot&nbsp;decision&nbsp;boundary
plt.title(&quot;Model&nbsp;with&nbsp;Momentum&nbsp;optimization&quot;)
axes&nbsp;=&nbsp;plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda&nbsp;x:&nbsp;
predict_dec(parameters,&nbsp;x.T),&nbsp;train_X,&nbsp;train_Y)</pre><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span><br/></p><p style="line-height:20px"><span style="font-size: 14px">结果：</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 0: 0.690741</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 1000: 0.685341</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 2000: 0.647145</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 3000: 0.619594</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 4000: 0.576665</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 5000: 0.607324</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 6000: 0.529476</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 7000: 0.460936</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 8000: 0.465780</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 9000: 0.464740</span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360497956725.jpg" title="1518360497956725.jpg" alt="1518360497956725.jpg" width="463" height="574"/></p><h3>5.3 - Mini-batch with Adam mode</h3><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Run the following code to see how the model does with Adam</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span></p><pre class="brush:python;toolbar:false">#&nbsp;train&nbsp;3-layer&nbsp;model
layers_dims&nbsp;=&nbsp;[train_X.shape[0],&nbsp;5,&nbsp;2,&nbsp;1]
parameters&nbsp;=&nbsp;model(train_X,&nbsp;train_Y,&nbsp;
layers_dims,&nbsp;optimizer&nbsp;=&nbsp;&quot;adam&quot;)
&nbsp;
#&nbsp;Predict
predictions&nbsp;=&nbsp;predict(train_X,&nbsp;train_Y,
&nbsp;parameters)
&nbsp;
#&nbsp;Plot&nbsp;decision&nbsp;boundary
plt.title(&quot;Model&nbsp;with&nbsp;Adam&nbsp;optimization&quot;)
axes&nbsp;=&nbsp;plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda&nbsp;x:&nbsp;
predict_dec(parameters,&nbsp;x.T),&nbsp;train_X,&nbsp;train_Y)</pre><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif"></span><br/></p><p style="line-height:20px"><span style="font-size: 14px">结果：</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 0: 0.690552</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 1000: 0.185567</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 2000: 0.150852</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 3000: 0.074454</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 4000: 0.125936</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 5000: 0.104235</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 6000: 0.100552</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 7000: 0.031601</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 8000: 0.111709</span></p><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Cost after epoch 9000: 0.197648</span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180211/1518360534252847.jpg" title="1518360534252847.jpg" alt="7.jpg"/></p><h3>5.4 – 总结</h3><table><tbody><tr class="firstRow"><td style="padding:0 0 0 0"><p><strong><span style="font-size: 13px;font-family: Helvetica, sans-serif">optimization method</span></strong> </p></td><td style="padding:0 0 0 0"><p><strong><span style="font-size: 13px;font-family: Helvetica, sans-serif">accuracy</span></strong> </p></td><td style="padding:0 0 0 0"><p><strong><span style="font-size: 13px;font-family: Helvetica, sans-serif">cost shape</span></strong> </p></td></tr><tr><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">Gradient descent </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">79.7% </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">oscillations </span></p></td></tr><tr><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">Momentum </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">79.7% </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">oscillations </span></p></td></tr><tr><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">Adam </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">94% </span></p></td><td style="padding:0 0 0 0"><p><span style="font-size: 13px;font-family: Helvetica, sans-serif">smoother</span></p></td></tr></tbody></table><p style="line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">&nbsp;&nbsp;&nbsp; </span><span style="font-size: 14px">动量通常有用，但是考虑到较小的学习速率和简单的数据库，它的影响几乎是微乎其微的。另外，考虑到许多</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">mini-batch</span><span style="font-size: 14px">比其它优化方法更困难这个事实，从这里面的出来的代价可以看到有很大的振荡。</span></p><p style="text-indent:28px;line-height:20px"><span style="font-size: 14px">在另一方面，</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">Adam</span><span style="font-size: 14px">明显优于</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">mini-batch</span><span style="font-size: 14px">和动量梯度下降算法。如果你在这个简单的数据集上运行更多时间段的模型，那么所有这些方法都会有非常好的记过。然而你可以看到，</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">Adam</span><span style="font-size: 14px">收敛更快。</span></p><p style="text-indent:28px;line-height:20px"><span style="font-size: 14px;font-family: Helvetica, sans-serif">Adam</span><span style="font-size: 14px">有以下优点：</span></p><p style="line-height:20px"><span style="font-size: 14px">（</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">1</span><span style="font-size: 14px">）相对较低的内存需求（虽然比梯度下降和动量梯度下降要求高）</span></p><p style="line-height:20px"><span style="font-size: 14px">（</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">2</span><span style="font-size: 14px">）即使稍微调整超参数（除了</span><span style="font-size: 14px;font-family: Helvetica, sans-serif">α</span><span style="font-size: 14px">外），也会工作效果很好。</span></p><p><br/></p>