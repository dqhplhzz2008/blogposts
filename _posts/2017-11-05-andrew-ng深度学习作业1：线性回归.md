---
ID: 3529
post_title: >
  Andrew
  Ng深度学习作业1：线性回归
post_name: 'andrew-ng%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e4%bd%9c%e4%b8%9a1%ef%bc%9a%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92'
author: 小奥
post_date: 2017-11-05 21:41:24
layout: post
link: >
  http://www.yushuai.me/2017/11/05/3529.html
published: true
tags: [ ]
categories:
  - Deep Learning
---
<pre class="brush:python;toolbar:false">%%&nbsp;Machine&nbsp;Learning&nbsp;Online&nbsp;Class&nbsp;-&nbsp;Exercise&nbsp;1:&nbsp;Linear&nbsp;Regression

%&nbsp;&nbsp;Instructions
%&nbsp;&nbsp;------------
%&nbsp;
%&nbsp;&nbsp;This&nbsp;file&nbsp;contains&nbsp;code&nbsp;that&nbsp;helps&nbsp;you&nbsp;get&nbsp;started&nbsp;on&nbsp;the
%&nbsp;&nbsp;linear&nbsp;exercise.&nbsp;You&nbsp;will&nbsp;need&nbsp;to&nbsp;complete&nbsp;the&nbsp;following&nbsp;functions&nbsp;
%&nbsp;&nbsp;in&nbsp;this&nbsp;exericse:
%
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;warmUpExercise.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;plotData.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gradientDescent.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;computeCost.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;gradientDescentMulti.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;computeCostMulti.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;featureNormalize.m
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;normalEqn.m
%
%&nbsp;&nbsp;For&nbsp;this&nbsp;exercise,&nbsp;you&nbsp;will&nbsp;not&nbsp;need&nbsp;to&nbsp;change&nbsp;any&nbsp;code&nbsp;in&nbsp;this&nbsp;file,
%&nbsp;&nbsp;or&nbsp;any&nbsp;other&nbsp;files&nbsp;other&nbsp;than&nbsp;those&nbsp;mentioned&nbsp;above.
%
%&nbsp;x&nbsp;refers&nbsp;to&nbsp;the&nbsp;population&nbsp;size&nbsp;in&nbsp;10,000s
%&nbsp;y&nbsp;refers&nbsp;to&nbsp;the&nbsp;profit&nbsp;in&nbsp;$10,000s
%

%%&nbsp;Initialization
clear&nbsp;all;&nbsp;close&nbsp;all;&nbsp;clc

%%&nbsp;====================&nbsp;Part&nbsp;1:&nbsp;Basic&nbsp;Function&nbsp;====================
%&nbsp;Complete&nbsp;warmUpExercise.m&nbsp;
fprintf(&#39;Running&nbsp;warmUpExercise&nbsp;...&nbsp;\n&#39;);
fprintf(&#39;5x5&nbsp;Identity&nbsp;Matrix:&nbsp;\n&#39;);
warmUpExercise()

fprintf(&#39;Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n&#39;);
pause;


%%&nbsp;=======================&nbsp;Part&nbsp;2:&nbsp;Plotting&nbsp;=======================
fprintf(&#39;Plotting&nbsp;Data&nbsp;...\n&#39;)
data&nbsp;=&nbsp;csvread(&#39;ex1data1.txt&#39;);
X&nbsp;=&nbsp;data(:,&nbsp;1);&nbsp;y&nbsp;=&nbsp;data(:,&nbsp;2);
m&nbsp;=&nbsp;length(y);&nbsp;%&nbsp;number&nbsp;of&nbsp;training&nbsp;examples

%&nbsp;Plot&nbsp;Data
%&nbsp;Note:&nbsp;You&nbsp;have&nbsp;to&nbsp;complete&nbsp;the&nbsp;code&nbsp;in&nbsp;plotData.m
plotData(X,&nbsp;y);

fprintf(&#39;Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n&#39;);
pause;

%%&nbsp;===================&nbsp;Part&nbsp;3:&nbsp;Gradient&nbsp;descent&nbsp;===================
fprintf(&#39;Running&nbsp;Gradient&nbsp;Descent&nbsp;...\n&#39;)

X&nbsp;=&nbsp;[ones(m,&nbsp;1),&nbsp;data(:,1)];&nbsp;%&nbsp;Add&nbsp;a&nbsp;column&nbsp;of&nbsp;ones&nbsp;to&nbsp;x，
%前面之所以生成一个1列全为1的列向量是因为要和theta0相乘，否则的话没办法相乘
theta&nbsp;=&nbsp;zeros(2,&nbsp;1);&nbsp;%&nbsp;initialize&nbsp;fitting&nbsp;parameters

%&nbsp;Some&nbsp;gradient&nbsp;descent&nbsp;settings
iterations&nbsp;=&nbsp;1500;
alpha&nbsp;=&nbsp;0.01;

%&nbsp;compute&nbsp;and&nbsp;display&nbsp;initial&nbsp;cost
computeCost(X,&nbsp;y,&nbsp;theta)

%&nbsp;run&nbsp;gradient&nbsp;descent
theta&nbsp;=&nbsp;gradientDescent(X,&nbsp;y,&nbsp;theta,&nbsp;alpha,&nbsp;iterations);

%&nbsp;print&nbsp;theta&nbsp;to&nbsp;screen
fprintf(&#39;Theta&nbsp;found&nbsp;by&nbsp;gradient&nbsp;descent:&nbsp;&#39;);
fprintf(&#39;%f&nbsp;%f&nbsp;\n&#39;,&nbsp;theta(1),&nbsp;theta(2));

%&nbsp;Plot&nbsp;the&nbsp;linear&nbsp;fit
hold&nbsp;on;&nbsp;%&nbsp;keep&nbsp;previous&nbsp;plot&nbsp;visible
plot(X(:,2),&nbsp;X*theta,&nbsp;&#39;-&#39;)
legend(&#39;Training&nbsp;data&#39;,&nbsp;&#39;Linear&nbsp;regression&#39;)
hold&nbsp;off&nbsp;%&nbsp;don&#39;t&nbsp;overlay&nbsp;any&nbsp;more&nbsp;plots&nbsp;on&nbsp;this&nbsp;figure

%&nbsp;Predict&nbsp;values&nbsp;for&nbsp;population&nbsp;sizes&nbsp;of&nbsp;35,000&nbsp;and&nbsp;70,000
predict1&nbsp;=&nbsp;[1,&nbsp;3.5]&nbsp;*theta;
fprintf(&#39;For&nbsp;population&nbsp;=&nbsp;35,000,&nbsp;we&nbsp;predict&nbsp;a&nbsp;profit&nbsp;of&nbsp;%f\n&#39;,...
&nbsp;&nbsp;&nbsp;&nbsp;predict1*10000);
predict2&nbsp;=&nbsp;[1,&nbsp;7]&nbsp;*&nbsp;theta;
fprintf(&#39;For&nbsp;population&nbsp;=&nbsp;70,000,&nbsp;we&nbsp;predict&nbsp;a&nbsp;profit&nbsp;of&nbsp;%f\n&#39;,...
&nbsp;&nbsp;&nbsp;&nbsp;predict2*10000);

fprintf(&#39;Program&nbsp;paused.&nbsp;Press&nbsp;enter&nbsp;to&nbsp;continue.\n&#39;);
pause;

%%&nbsp;=============&nbsp;Part&nbsp;4:&nbsp;Visualizing&nbsp;J(theta_0,&nbsp;theta_1)&nbsp;=============
fprintf(&#39;Visualizing&nbsp;J(theta_0,&nbsp;theta_1)&nbsp;...\n&#39;)

%&nbsp;Grid&nbsp;over&nbsp;which&nbsp;we&nbsp;will&nbsp;calculate&nbsp;J
theta0_vals&nbsp;=&nbsp;linspace(-10,&nbsp;10,&nbsp;100);
theta1_vals&nbsp;=&nbsp;linspace(-1,&nbsp;4,&nbsp;100);

%&nbsp;initialize&nbsp;J_vals&nbsp;to&nbsp;a&nbsp;matrix&nbsp;of&nbsp;0&#39;s
J_vals&nbsp;=&nbsp;zeros(length(theta0_vals),&nbsp;length(theta1_vals));

%&nbsp;Fill&nbsp;out&nbsp;J_vals
for&nbsp;i&nbsp;=&nbsp;1:length(theta0_vals)
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;j&nbsp;=&nbsp;1:length(theta1_vals)
	&nbsp;&nbsp;t&nbsp;=&nbsp;[theta0_vals(i);&nbsp;theta1_vals(j)];&nbsp;&nbsp;&nbsp;&nbsp;
	&nbsp;&nbsp;J_vals(i,j)&nbsp;=&nbsp;computeCost(X,&nbsp;y,&nbsp;t);
&nbsp;&nbsp;&nbsp;&nbsp;end
end


%&nbsp;Because&nbsp;of&nbsp;the&nbsp;way&nbsp;meshgrids&nbsp;work&nbsp;in&nbsp;the&nbsp;surf&nbsp;command,&nbsp;we&nbsp;need&nbsp;to&nbsp;
%&nbsp;transpose&nbsp;J_vals&nbsp;before&nbsp;calling&nbsp;surf,&nbsp;or&nbsp;else&nbsp;the&nbsp;axes&nbsp;will&nbsp;be&nbsp;flipped
J_vals&nbsp;=&nbsp;J_vals&#39;;
%&nbsp;Surface&nbsp;plot
figure;
surf(theta0_vals,&nbsp;theta1_vals,&nbsp;J_vals)
xlabel(&#39;\theta_0&#39;);&nbsp;ylabel(&#39;\theta_1&#39;);

%&nbsp;Contour&nbsp;plot
figure;
%&nbsp;Plot&nbsp;J_vals&nbsp;as&nbsp;15&nbsp;contours&nbsp;spaced&nbsp;logarithmically&nbsp;between&nbsp;0.01&nbsp;and&nbsp;100
contour(theta0_vals,&nbsp;theta1_vals,&nbsp;J_vals,&nbsp;logspace(-2,&nbsp;3,&nbsp;20))
xlabel(&#39;\theta_0&#39;);&nbsp;ylabel(&#39;\theta_1&#39;);
hold&nbsp;on;
plot(theta(1),&nbsp;theta(2),&nbsp;&#39;rx&#39;,&nbsp;&#39;MarkerSize&#39;,&nbsp;10,&nbsp;&#39;LineWidth&#39;,&nbsp;2);</pre><p>ComputeCost.m<br/></p><pre class="brush:python;toolbar:false">function&nbsp;J&nbsp;=&nbsp;computeCost(X,&nbsp;y,&nbsp;theta)
%COMPUTECOST&nbsp;Compute&nbsp;cost&nbsp;for&nbsp;linear&nbsp;regression
%&nbsp;&nbsp;&nbsp;J&nbsp;=&nbsp;COMPUTECOST(X,&nbsp;y,&nbsp;theta)&nbsp;computes&nbsp;the&nbsp;cost&nbsp;of&nbsp;using&nbsp;theta&nbsp;as&nbsp;the
%&nbsp;&nbsp;&nbsp;parameter&nbsp;for&nbsp;linear&nbsp;regression&nbsp;to&nbsp;fit&nbsp;the&nbsp;data&nbsp;points&nbsp;in&nbsp;X&nbsp;and&nbsp;y
%&nbsp;Initialize&nbsp;some&nbsp;useful&nbsp;values
m&nbsp;=&nbsp;length(y);&nbsp;%&nbsp;number&nbsp;of&nbsp;training&nbsp;examples
%&nbsp;You&nbsp;need&nbsp;to&nbsp;return&nbsp;the&nbsp;following&nbsp;variables&nbsp;correctly&nbsp;
J&nbsp;=&nbsp;0;
%&nbsp;======================&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;======================
%&nbsp;Instructions:&nbsp;Compute&nbsp;the&nbsp;cost&nbsp;of&nbsp;a&nbsp;particular&nbsp;choice&nbsp;of&nbsp;theta
%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;should&nbsp;set&nbsp;J&nbsp;to&nbsp;the&nbsp;cost.
%之所以是X*theta是因为假设的h-theta(x)=theta0+theta1*x1
J&nbsp;=&nbsp;sum((X*theta&nbsp;-&nbsp;y).^2)&nbsp;/&nbsp;(2&nbsp;*&nbsp;m);
%&nbsp;=========================================================================
end</pre><p>gradientDescent.m</p><pre class="brush:python;toolbar:false">function&nbsp;[theta,&nbsp;J_history]&nbsp;=&nbsp;gradientDescent(X,&nbsp;y,&nbsp;theta,&nbsp;alpha,&nbsp;num_iters)
%GRADIENTDESCENT&nbsp;Performs&nbsp;gradient&nbsp;descent&nbsp;to&nbsp;learn&nbsp;theta
%&nbsp;&nbsp;&nbsp;theta&nbsp;=&nbsp;GRADIENTDESENT(X,&nbsp;y,&nbsp;theta,&nbsp;alpha,&nbsp;num_iters)&nbsp;updates&nbsp;theta&nbsp;by&nbsp;
%&nbsp;&nbsp;&nbsp;taking&nbsp;num_iters&nbsp;gradient&nbsp;steps&nbsp;with&nbsp;learning&nbsp;rate&nbsp;alpha
%&nbsp;Initialize&nbsp;some&nbsp;useful&nbsp;values
m&nbsp;=&nbsp;length(y);&nbsp;%&nbsp;number&nbsp;of&nbsp;training&nbsp;examples
J_history&nbsp;=&nbsp;zeros(num_iters,&nbsp;1);
for&nbsp;iter&nbsp;=&nbsp;1:num_iters
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;======================&nbsp;YOUR&nbsp;CODE&nbsp;HERE&nbsp;======================
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;Instructions:&nbsp;Perform&nbsp;a&nbsp;single&nbsp;gradient&nbsp;step&nbsp;on&nbsp;the&nbsp;parameter&nbsp;vector
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;theta.&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;%
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;Hint:&nbsp;While&nbsp;debugging,&nbsp;it&nbsp;can&nbsp;be&nbsp;useful&nbsp;to&nbsp;print&nbsp;out&nbsp;the&nbsp;values
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;the&nbsp;cost&nbsp;function&nbsp;(computeCost)&nbsp;and&nbsp;gradient&nbsp;here.
&nbsp;&nbsp;&nbsp;&nbsp;%
H&nbsp;=&nbsp;X&nbsp;*&nbsp;theta;
T&nbsp;=&nbsp;[0&nbsp;;&nbsp;0];
for&nbsp;i&nbsp;=&nbsp;1&nbsp;:&nbsp;m,
T&nbsp;=&nbsp;T&nbsp;+&nbsp;(H(i)&nbsp;-&nbsp;y(i))&nbsp;*&nbsp;X(i,:)&#39;;
end
theta&nbsp;=&nbsp;theta&nbsp;-&nbsp;(alpha&nbsp;*&nbsp;T)&nbsp;/&nbsp;m;
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;============================================================
&nbsp;&nbsp;&nbsp;&nbsp;%&nbsp;Save&nbsp;the&nbsp;cost&nbsp;J&nbsp;in&nbsp;every&nbsp;iteration&nbsp;&nbsp;&nbsp;&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;J_history(iter)&nbsp;=&nbsp;computeCost(X,&nbsp;y,&nbsp;theta);
end
end</pre><p><br/></p>