---
ID: 3551
post_title: >
  吴恩达深度学习课程
  DeepLearning.ai
  编程作业（1-2）Part.2
post_name: '%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8b-deeplearning-ai-%e7%bc%96%e7%a8%8b%e4%bd%9c%e4%b8%9a%ef%bc%88%e7%ac%ac%e4%ba%8c%e5%91%a8%ef%bc%89%ef%bc%882%ef%bc%89'
author: 小奥
post_date: 2018-01-30 18:11:57
layout: post
link: >
  http://www.yushuai.me/2018/01/30/3551.html
published: true
tags:
  - Python
  - 深度学习
  - 神经网络
categories:
  - Deep Learning
---
<h2 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">3 - General Architecture of the learning algorithm</span></h2><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">It</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">’s time to design a simple algorithm to distinguish cat images from non-cat images.</span></p><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why&nbsp;<strong style="box-sizing: border-box">Logistic Regression is actually a very simple Neural Network!</strong></span></p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180130/1517307089278628.png" alt="nnc.png" title="1517307089278628.png" width="554" height="430"/></p><p><strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F;background:white">Key steps</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F;background:white">:&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F"><br/> <span style="background:white">In this exercise, you will carry out the following steps:&nbsp;</span><br/> <span style="background:white">- Initialize the parameters of the model&nbsp;</span><br/> <span style="background:white">- Learn the parameters for the model by minimizing the cost&nbsp;</span><br/> <span style="background:white">- Use the learned parameters to make predictions (on the test set)&nbsp;</span><br/> <span style="background:white">- Analyse the results and conclude</span></span></p><h2 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">4 - Building the parts of our algorithm ##</span></h2><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">The main steps for building a Neural Network are:&nbsp;<br/> 1. Define the model structure (such as number of input features)&nbsp;<br/> 2. Initialize the model</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">’s parameters&nbsp;<br/> 3. Loop:&nbsp;<br/> - Calculate current loss (forward propagation)&nbsp;<br/> - Calculate current gradient (backward propagation)&nbsp;<br/> - Update parameters (gradient descent)</span></p><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">You often build 1-3 separately and integrate them into one function we call&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">model()</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">.</span></p><p><br/></p><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">.1 - Helper functions</span></h3><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><strong style="box-sizing: border-box"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">: Using your code from </span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">“Python Basics”, implement&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">sigmoid()</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">.&nbsp;</span></p><pre class="brush:python;toolbar:false">#1&nbsp;GRADED&nbsp;FUNCTION:&nbsp;sigmoid
def&nbsp;sigmoid(z):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Compute&nbsp;the&nbsp;sigmoid&nbsp;of&nbsp;z
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;z&nbsp;--&nbsp;A&nbsp;scalar&nbsp;or&nbsp;numpy&nbsp;array&nbsp;of&nbsp;any&nbsp;size.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Return:
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;--&nbsp;sigmoid(z)
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;s&nbsp;=&nbsp;1.0/(1+np.exp(-z))
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s
&nbsp;
print&nbsp;(&quot;sigmoid([0,&nbsp;2])&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(sigmoid(np.array([0,2]))))</pre><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">4.2 - Initializing parameters</span></h3><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><strong style="box-sizing: border-box"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don</span><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</span></p><pre class="brush:python;toolbar:false">#2Initializing&nbsp;parameters
def&nbsp;initialize_with_zeros(dim):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;function&nbsp;creates&nbsp;a&nbsp;vector&nbsp;of&nbsp;zeros&nbsp;of&nbsp;shape&nbsp;(dim,&nbsp;1)&nbsp;for&nbsp;w&nbsp;and&nbsp;initializes&nbsp;b&nbsp;to&nbsp;0.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Argument:
&nbsp;&nbsp;&nbsp;&nbsp;dim&nbsp;--&nbsp;size&nbsp;of&nbsp;the&nbsp;w&nbsp;vector&nbsp;we&nbsp;want&nbsp;(or&nbsp;number&nbsp;of&nbsp;parameters&nbsp;in&nbsp;this&nbsp;case)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;--&nbsp;initialized&nbsp;vector&nbsp;of&nbsp;shape&nbsp;(dim,&nbsp;1)
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;--&nbsp;initialized&nbsp;scalar&nbsp;(corresponds&nbsp;to&nbsp;the&nbsp;bias)
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(≈&nbsp;1&nbsp;line&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;=&nbsp;np.zeros((dim,&nbsp;1))
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
#python&nbsp;assert断言是声明其布尔值必须为真的判定，如果发生异常就说明表达示为假。可以
#理解assert断言语句为raise-if-not，用来测试表示式，其返回值为假，就会触发异常
&nbsp;&nbsp;&nbsp;&nbsp;assert(w.shape&nbsp;==&nbsp;(dim,&nbsp;1))
&nbsp;&nbsp;&nbsp;&nbsp;assert(isinstance(b,&nbsp;float)&nbsp;or&nbsp;isinstance(b,&nbsp;int))#判断b是int或者是float
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;w,&nbsp;b
&nbsp;
dim&nbsp;=&nbsp;2
w,&nbsp;b&nbsp;=&nbsp;initialize_with_zeros(dim)
print&nbsp;(&quot;w&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(w))
print&nbsp;(&quot;b&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(b))</pre><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">4.3 - Forward and Backward propagation</span></h3><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Now that your parameters are initialized, you can do the </span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">“forward” and “backward” propagation steps for learning the parameters.</span></p><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><strong style="box-sizing: border-box"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Implement a function&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">propagate()</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">that computes the cost function and its gradient.</span></p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;propagate
&nbsp;
def&nbsp;propagate(w,&nbsp;b,&nbsp;X,&nbsp;Y):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Implement&nbsp;the&nbsp;cost&nbsp;function&nbsp;and&nbsp;its&nbsp;gradient&nbsp;for&nbsp;the&nbsp;propagation&nbsp;explained&nbsp;above
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;--&nbsp;weights,&nbsp;a&nbsp;numpy&nbsp;array&nbsp;of&nbsp;size&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;1)
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;--&nbsp;bias,&nbsp;a&nbsp;scalar
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;--&nbsp;data&nbsp;of&nbsp;size&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;Y&nbsp;--&nbsp;true&nbsp;&quot;label&quot;&nbsp;vector&nbsp;(containing&nbsp;0&nbsp;if&nbsp;non-cat,&nbsp;1&nbsp;if&nbsp;cat)&nbsp;of&nbsp;size&nbsp;(1,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Return:
&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;--&nbsp;negative&nbsp;log-likelihood&nbsp;cost&nbsp;for&nbsp;logistic&nbsp;regression
&nbsp;&nbsp;&nbsp;&nbsp;dw&nbsp;--&nbsp;gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;w,&nbsp;thus&nbsp;same&nbsp;shape&nbsp;as&nbsp;w
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;--&nbsp;gradient&nbsp;of&nbsp;the&nbsp;loss&nbsp;with&nbsp;respect&nbsp;to&nbsp;b,&nbsp;thus&nbsp;same&nbsp;shape&nbsp;as&nbsp;b
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Tips:
&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;Write&nbsp;your&nbsp;code&nbsp;step&nbsp;by&nbsp;step&nbsp;for&nbsp;the&nbsp;propagation.&nbsp;np.log(),&nbsp;np.dot()
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;m&nbsp;=&nbsp;X.shape[1]
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;FORWARD&nbsp;PROPAGATION&nbsp;(FROM&nbsp;X&nbsp;TO&nbsp;COST)
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(≈&nbsp;2&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;=&nbsp;sigmoid(np.dot(w.T,X)+b)&nbsp;#&nbsp;compute&nbsp;activation
&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;=&nbsp;-(1.0/m)*np.sum(Y*np.log(A)+(1-Y)*np.log(1-A))&nbsp;#&nbsp;compute&nbsp;cost
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;BACKWARD&nbsp;PROPAGATION&nbsp;(TO&nbsp;FIND&nbsp;GRAD)
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(≈&nbsp;2&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;dw&nbsp;=&nbsp;(1.0/m)*np.dot(X,(A-Y).T)
&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;=&nbsp;(1.0/m)*np.sum(A-Y)
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;assert(dw.shape&nbsp;==&nbsp;w.shape)
&nbsp;&nbsp;&nbsp;&nbsp;assert(db.dtype&nbsp;==&nbsp;float)
&nbsp;&nbsp;&nbsp;&nbsp;cost&nbsp;=&nbsp;np.squeeze(cost)
&nbsp;&nbsp;&nbsp;&nbsp;assert(cost.shape&nbsp;==&nbsp;())
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;=&nbsp;{&quot;dw&quot;:&nbsp;dw,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;db&quot;:&nbsp;db}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;grads,&nbsp;cost
&nbsp;
w,&nbsp;b,&nbsp;X,&nbsp;Y&nbsp;=&nbsp;np.array([[1.],[2.]]),&nbsp;2.,&nbsp;np.array([[1.,2.,-1.],[3.,4.,-3.2]]),&nbsp;np.array([[1,0,1]])
grads,&nbsp;cost&nbsp;=&nbsp;propagate(w,&nbsp;b,&nbsp;X,&nbsp;Y)
print&nbsp;(&quot;dw&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(grads[&quot;dw&quot;]))
print&nbsp;(&quot;db&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(grads[&quot;db&quot;]))
print&nbsp;(&quot;cost&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(cost))</pre><h3 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:30px;background:white"><span style="font-size: 22px;font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">d) Optimization</span></h3><p style="margin: 8px 0 0 32px;background: white"><span style="font-size:13px;font-family:Symbol;color:#454545">·<span style="font-variant-numeric: normal;font-stretch: normal;font-size: 9px;line-height: normal;font-family: &#39;Times New Roman&#39;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#454545">You have initialized your parameters.</span></p><p style="margin: 8px 0 0 32px;background: white"><span style="font-size:13px;font-family:Symbol;color:#454545">·<span style="font-variant-numeric: normal;font-stretch: normal;font-size: 9px;line-height: normal;font-family: &#39;Times New Roman&#39;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#454545">You are also able to compute a cost function and its gradient.</span></p><p style="margin: 8px 0 0 32px;background: white"><span style="font-size:13px;font-family:Symbol;color:#454545">·<span style="font-variant-numeric: normal;font-stretch: normal;font-size: 9px;line-height: normal;font-family: &#39;Times New Roman&#39;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#454545">Now, you want to update the parameters using gradient descent.</span></p><p style=";text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><strong style="box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Write down the optimization function. The goal is to learn&nbsp;</span><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.952em 1000em 2.702em -0.447em)">w</span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style: initial;border-color:transparent !important;border-image: initial"></span></span></span></span>&nbsp;<span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">and&nbsp;</span><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.702em 1000em 2.702em -0.398em)">b</span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style: initial;border-color:transparent !important;border-image: initial"></span></span></span></span>&nbsp;<span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">by minimizing the cost function&nbsp;</span><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.702em 1000em 2.702em -0.398em)"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial">J</span></span></span></span></span></span>. For a parameter&nbsp;<span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.702em 1000em 2.702em -0.398em)">θ</span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style: initial;border-color:transparent !important;border-image: initial"></span></span></span></span>, the update rule is&nbsp;<span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.852em 1000em 2.853em -0.398em)"><span style="box-sizing: border-box;transition: none"><span style="box-sizing: border-box;transition: none">θ</span></span><span style="font-size: 20px;font-family: MathJax_Main, serif;border: 1px none windowtext;padding: 0">=</span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">θ</span><span style="font-size: 20px;font-family: MathJax_Main, serif;border: 1px none windowtext;padding: 0">−</span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">α</span><span style="font-size: 20px;font-family: MathJax_Main, serif;border: 1px none windowtext;padding: 0">&nbsp;</span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">d</span><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial">θ</span></span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style:initial;border-color:transparent !important;border-image: initial"></span></span></span></span>, where&nbsp;<span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.952em 1000em 2.702em -0.398em)">α</span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style: initial;border-color:transparent !important;border-image: initial"></span></span></span></span>&nbsp;<span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">is the learning rate.</span></p><p># GRADED FUNCTION: optimize</p><p>&nbsp;</p><pre class="brush:python;toolbar:false">def&nbsp;optimize(w,&nbsp;b,&nbsp;X,&nbsp;Y,&nbsp;num_iterations,&nbsp;learning_rate,&nbsp;print_cost&nbsp;=&nbsp;False):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;This&nbsp;function&nbsp;optimizes&nbsp;w&nbsp;and&nbsp;b&nbsp;by&nbsp;running&nbsp;a&nbsp;gradient&nbsp;descent&nbsp;algorithm
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;--&nbsp;weights,&nbsp;a&nbsp;numpy&nbsp;array&nbsp;of&nbsp;size&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;1)
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;--&nbsp;bias,&nbsp;a&nbsp;scalar
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;--&nbsp;data&nbsp;of&nbsp;shape&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;Y&nbsp;--&nbsp;true&nbsp;&quot;label&quot;&nbsp;vector&nbsp;(containing&nbsp;0&nbsp;if&nbsp;non-cat,&nbsp;1&nbsp;if&nbsp;cat),&nbsp;of&nbsp;shape&nbsp;(1,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;&nbsp;&nbsp;&nbsp;num_iterations&nbsp;--&nbsp;number&nbsp;of&nbsp;iterations&nbsp;of&nbsp;the&nbsp;optimization&nbsp;loop
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;--&nbsp;learning&nbsp;rate&nbsp;of&nbsp;the&nbsp;gradient&nbsp;descent&nbsp;update&nbsp;rule
&nbsp;&nbsp;&nbsp;&nbsp;print_cost&nbsp;--&nbsp;True&nbsp;to&nbsp;print&nbsp;the&nbsp;loss&nbsp;every&nbsp;100&nbsp;steps
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;params&nbsp;--&nbsp;dictionary&nbsp;containing&nbsp;the&nbsp;weights&nbsp;w&nbsp;and&nbsp;bias&nbsp;b
&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;--&nbsp;dictionary&nbsp;containing&nbsp;the&nbsp;gradients&nbsp;of&nbsp;the&nbsp;weights&nbsp;and&nbsp;bias&nbsp;with&nbsp;respect&nbsp;to&nbsp;the&nbsp;cost&nbsp;function
&nbsp;&nbsp;&nbsp;&nbsp;costs&nbsp;--&nbsp;list&nbsp;of&nbsp;all&nbsp;the&nbsp;costs&nbsp;computed&nbsp;during&nbsp;the&nbsp;optimization,&nbsp;this&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;plot&nbsp;the&nbsp;learning&nbsp;curve.
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Tips:
&nbsp;&nbsp;&nbsp;&nbsp;You&nbsp;basically&nbsp;need&nbsp;to&nbsp;write&nbsp;down&nbsp;two&nbsp;steps&nbsp;and&nbsp;iterate&nbsp;through&nbsp;them:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1)&nbsp;Calculate&nbsp;the&nbsp;cost&nbsp;and&nbsp;the&nbsp;gradient&nbsp;for&nbsp;the&nbsp;current&nbsp;parameters.&nbsp;Use&nbsp;propagate().
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2)&nbsp;Update&nbsp;the&nbsp;parameters&nbsp;using&nbsp;gradient&nbsp;descent&nbsp;rule&nbsp;for&nbsp;w&nbsp;and&nbsp;b.
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;costs&nbsp;=&nbsp;[]
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(num_iterations):
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Cost&nbsp;and&nbsp;gradient&nbsp;calculation&nbsp;(≈&nbsp;1-4&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;grads,&nbsp;cost&nbsp;=&nbsp;propagate(w,&nbsp;b,&nbsp;X,&nbsp;Y)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Retrieve&nbsp;derivatives&nbsp;from&nbsp;grads
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dw&nbsp;=&nbsp;grads[&quot;dw&quot;]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;db&nbsp;=&nbsp;grads[&quot;db&quot;]
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;update&nbsp;rule&nbsp;(≈&nbsp;2&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;=&nbsp;w&nbsp;-&nbsp;learning_rate*dw
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;b&nbsp;-&nbsp;learning_rate*db
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Record&nbsp;the&nbsp;costs
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;i&nbsp;%&nbsp;100&nbsp;==&nbsp;0:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;costs.append(cost)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Print&nbsp;the&nbsp;cost&nbsp;every&nbsp;100&nbsp;training&nbsp;examples
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;print_cost&nbsp;and&nbsp;i&nbsp;%&nbsp;100&nbsp;==&nbsp;0:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print&nbsp;(&quot;Cost&nbsp;after&nbsp;iteration&nbsp;%i:&nbsp;%f&quot;&nbsp;%(i,&nbsp;cost))
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;params&nbsp;=&nbsp;{&quot;w&quot;:&nbsp;w,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;b&quot;:&nbsp;b}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;grads&nbsp;=&nbsp;{&quot;dw&quot;:&nbsp;dw,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;db&quot;:&nbsp;db}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;params,&nbsp;grads,&nbsp;costs
&nbsp;
params,&nbsp;grads,&nbsp;costs&nbsp;=&nbsp;optimize(w,&nbsp;b,&nbsp;X,&nbsp;Y,&nbsp;num_iterations=&nbsp;100,&nbsp;learning_rate&nbsp;=&nbsp;0.009,&nbsp;print_cost&nbsp;=&nbsp;False)
&nbsp;
print&nbsp;(&quot;w&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(params[&quot;w&quot;]))
print&nbsp;(&quot;b&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(params[&quot;b&quot;]))
print&nbsp;(&quot;dw&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(grads[&quot;dw&quot;]))
print&nbsp;(&quot;db&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(grads[&quot;db&quot;]))</pre><p style="margin-top:0;margin-right:0;margin-bottom:16px;margin-left: 0;text-align:justify;text-justify:inter-ideograph;line-height:26px;background:white"><strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">predict()</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">function. There is two steps to computing predictions:</span></p><p style="margin-top:0;margin-right:0;margin-bottom:0;margin-left:40px;margin-bottom:0;text-align:justify;text-justify:inter-ideograph;line-height:26px;background:white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">1.<span style="font-variant-numeric: normal;font-stretch: normal;font-size: 9px;line-height: normal;font-family: &#39;Times New Roman&#39;">&nbsp;&nbsp;&nbsp; </span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Calculate&nbsp;</span><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="font-size:20px;font-family:&#39;MathJax_Math-italic&#39;,serif;color:#4F4F4F;border:none windowtext 1px;padding:0"><span style="box-sizing: border-box;transition: none;display:inline-block"><span style="box-sizing: border-box;word-wrap: normal;max-width:none;max-height: none;min-width: 0px;min-height: 0px;float:none;word-spacing:normal"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.602em 1000em 3.103em -0.398em)"><span style="box-sizing: border-box;transition: none"><span style="box-sizing: border-box;transition: none"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial"><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial"><span style="box-sizing: border-box;transition: none;display:inline-block;border-style:initial;border-color:transparent !important;border-image: initial"><span style="box-sizing: border-box;transition: none;clip:rect(1.702em 1000em 2.702em -0.398em)"><span style="box-sizing: border-box;transition: none">Y</span></span><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial;clip:rect(1.852em 1000em 2.302em -0.348em)"><span style="box-sizing: border-box;transition: none">^</span></span></span></span></span></span><span style="box-sizing: border-box;transition: none">=</span></span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">A</span><span style="font-size: 20px;font-family: MathJax_Main, serif;border: 1px none windowtext;padding: 0">=</span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">σ</span><span style="box-sizing: border-box;transition: none">(</span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style: initial;border-color:transparent !important;border-image: initial"><span style="font-size: 20px;border: 1px none windowtext;padding: 0"><span style="box-sizing: border-box;transition: none;clip:rect(1.952em 1000em 2.702em -0.447em)"><span style="box-sizing: border-box;transition: none">w</span></span><span style="box-sizing: border-box;transition: none;border-style:initial;border-color:transparent !important;border-image: initial"><span style="box-sizing: border-box;transition: none">T</span></span></span></span><span style="box-sizing: border-box;transition: none">X</span><span style="box-sizing: border-box;transition: none">+</span><span style="font-size: 20px;border: 1px none windowtext;padding: 0">b</span><span style="font-size: 20px;font-family: MathJax_Main, serif;border: 1px none windowtext;padding: 0">)</span></span></span><span style="box-sizing: border-box;transition: none;display:inline-block;border-style:initial;border-color:transparent !important;border-image: initial"></span></span></span></span></p><p style="margin-top:0;margin-right:0;margin-bottom:16px;margin-left: 40px;text-align:justify;text-justify:inter-ideograph;line-height:26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">2.<span style="font-variant-numeric: normal;font-stretch: normal;font-size: 9px;line-height: normal;font-family: &#39;Times New Roman&#39;">&nbsp;&nbsp;&nbsp; </span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">Y_prediction</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">. If you wish, you can use an&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">if</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">/</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">else</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">statement in a&nbsp;</span><code style="box-sizing: border-box;border-radius: 4px"><span style="font-size:14px;font-family:Consolas;color:#C7254E;background:#F9F2F4">for</span></code><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">loop (though there is also a way to vectorize this).</span></p><pre class="brush:python;toolbar:false">#&nbsp;GRADED&nbsp;FUNCTION:&nbsp;predict
&nbsp;
def&nbsp;predict(w,&nbsp;b,&nbsp;X):
&nbsp;&nbsp;&nbsp;&nbsp;&#39;&#39;&#39;
&nbsp;&nbsp;&nbsp;&nbsp;Predict&nbsp;whether&nbsp;the&nbsp;label&nbsp;is&nbsp;0&nbsp;or&nbsp;1&nbsp;using&nbsp;learned&nbsp;logistic&nbsp;regression&nbsp;parameters&nbsp;(w,&nbsp;b)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;--&nbsp;weights,&nbsp;a&nbsp;numpy&nbsp;array&nbsp;of&nbsp;size&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;1)
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;--&nbsp;bias,&nbsp;a&nbsp;scalar
&nbsp;&nbsp;&nbsp;&nbsp;X&nbsp;--&nbsp;data&nbsp;of&nbsp;size&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;number&nbsp;of&nbsp;examples)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction&nbsp;--&nbsp;a&nbsp;numpy&nbsp;array&nbsp;(vector)&nbsp;containing&nbsp;all&nbsp;predictions&nbsp;(0/1)&nbsp;for&nbsp;the&nbsp;examples&nbsp;in&nbsp;X
&nbsp;&nbsp;&nbsp;&nbsp;&#39;&#39;&#39;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;m&nbsp;=&nbsp;X.shape[1]
&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction&nbsp;=&nbsp;np.zeros((1,m))
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;=&nbsp;w.reshape(X.shape[0],&nbsp;1)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Compute&nbsp;vector&nbsp;&quot;A&quot;&nbsp;predicting&nbsp;the&nbsp;probabilities&nbsp;of&nbsp;a&nbsp;cat&nbsp;being&nbsp;present&nbsp;in&nbsp;the&nbsp;picture
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(≈&nbsp;1&nbsp;line&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;=&nbsp;sigmoid(np.dot(w.T,&nbsp;X)&nbsp;+&nbsp;b)
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;i&nbsp;in&nbsp;range(A.shape[1]):
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Convert&nbsp;probabilities&nbsp;A[0,i]&nbsp;to&nbsp;actual&nbsp;predictions&nbsp;p[0,i]
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###&nbsp;(≈&nbsp;4&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;A[0,i]&nbsp;&gt;&nbsp;0.5:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction[0,i]&nbsp;=&nbsp;1
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction[0,i]&nbsp;=&nbsp;0
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;assert(Y_prediction.shape&nbsp;==&nbsp;(1,&nbsp;m))
&nbsp;
return&nbsp;Y_prediction
w&nbsp;=&nbsp;np.array([[0.1124579],[0.23106775]])
b&nbsp;=&nbsp;-0.3
X&nbsp;=&nbsp;np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]])
print&nbsp;(&quot;predictions&nbsp;=&nbsp;&quot;&nbsp;+&nbsp;str(predict(w,&nbsp;b,&nbsp;X)))</pre><p><strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:blue;background:white">What to remember:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:blue;background:white">&nbsp;</span><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:blue"><br/> <span style="background:white">You</span><span style="background: white">’ve implemented several functions that:&nbsp;</span></span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:blue"><br/> <span style="background:white">- Initialize (w,b)&nbsp;</span><br/> <span style="background:white">- Optimize the loss iteratively to learn parameters (w,b):&nbsp;</span><br/> <span style="background:white">- computing the cost and its gradient&nbsp;</span><br/> <span style="background:white">- updating the parameters using gradient descent&nbsp;</span><br/> <span style="background:white">- Use the learned (w,b) to predict the labels for a given set of examples</span></span></p><h2 style="margin-top:8px;margin-right:0;margin-bottom:16px;margin-left: 0;line-height:32px;background:white"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">5 - Merge all functions into a model</span></h2><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</span></p><p style="margin: 0 0 16px;text-align: justify;line-height: 26px;background: white;box-sizing: border-box"><strong style="box-sizing: border-box"><span style="font-family: &#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Exercise:</span></strong><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">&nbsp;</span><span style="font-family:&#39;微软雅黑&#39;,sans-serif;color:#4F4F4F">Implement the model function. Use the following notation:&nbsp;<br/> - Y_prediction for your predictions on the test set&nbsp;<br/> - Y_prediction_train for your predictions on the train set&nbsp;<br/> - w, costs, grads for the outputs of optimize()</span></p><pre class="brush:python;toolbar:false">def&nbsp;model(X_train,&nbsp;Y_train,&nbsp;X_test,&nbsp;Y_test,&nbsp;num_iterations&nbsp;=&nbsp;2000,&nbsp;learning_rate&nbsp;=&nbsp;0.5,&nbsp;print_cost&nbsp;=&nbsp;False):
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;&nbsp;&nbsp;&nbsp;Builds&nbsp;the&nbsp;logistic&nbsp;regression&nbsp;model&nbsp;by&nbsp;calling&nbsp;the&nbsp;function&nbsp;you&#39;ve&nbsp;implemented&nbsp;previously
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Arguments:
&nbsp;&nbsp;&nbsp;&nbsp;X_train&nbsp;--&nbsp;training&nbsp;set&nbsp;represented&nbsp;by&nbsp;a&nbsp;numpy&nbsp;array&nbsp;of&nbsp;shape&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;m_train)
&nbsp;&nbsp;&nbsp;&nbsp;Y_train&nbsp;--&nbsp;training&nbsp;labels&nbsp;represented&nbsp;by&nbsp;a&nbsp;numpy&nbsp;array&nbsp;(vector)&nbsp;of&nbsp;shape&nbsp;(1,&nbsp;m_train)
&nbsp;&nbsp;&nbsp;&nbsp;X_test&nbsp;--&nbsp;test&nbsp;set&nbsp;represented&nbsp;by&nbsp;a&nbsp;numpy&nbsp;array&nbsp;of&nbsp;shape&nbsp;(num_px&nbsp;*&nbsp;num_px&nbsp;*&nbsp;3,&nbsp;m_test)
&nbsp;&nbsp;&nbsp;&nbsp;Y_test&nbsp;--&nbsp;test&nbsp;labels&nbsp;represented&nbsp;by&nbsp;a&nbsp;numpy&nbsp;array&nbsp;(vector)&nbsp;of&nbsp;shape&nbsp;(1,&nbsp;m_test)
&nbsp;&nbsp;&nbsp;&nbsp;num_iterations&nbsp;--&nbsp;hyperparameter&nbsp;representing&nbsp;the&nbsp;number&nbsp;of&nbsp;iterations&nbsp;to&nbsp;optimize&nbsp;the&nbsp;parameters
&nbsp;&nbsp;&nbsp;&nbsp;learning_rate&nbsp;--&nbsp;hyperparameter&nbsp;representing&nbsp;the&nbsp;learning&nbsp;rate&nbsp;used&nbsp;in&nbsp;the&nbsp;update&nbsp;rule&nbsp;of&nbsp;optimize()
&nbsp;&nbsp;&nbsp;&nbsp;print_cost&nbsp;--&nbsp;Set&nbsp;to&nbsp;true&nbsp;to&nbsp;print&nbsp;the&nbsp;cost&nbsp;every&nbsp;100&nbsp;iterations
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;Returns:
&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;--&nbsp;dictionary&nbsp;containing&nbsp;information&nbsp;about&nbsp;the&nbsp;model.
&nbsp;&nbsp;&nbsp;&nbsp;&quot;&quot;&quot;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;START&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;initialize&nbsp;parameters&nbsp;with&nbsp;zeros&nbsp;(≈&nbsp;1&nbsp;line&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;w,&nbsp;b&nbsp;=&nbsp;initialize_with_zeros(X_train.shape[0])
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Gradient&nbsp;descent&nbsp;(≈&nbsp;1&nbsp;line&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;parameters,&nbsp;grads,&nbsp;costs&nbsp;=&nbsp;optimize(w,&nbsp;b,&nbsp;X_train,&nbsp;Y_train,&nbsp;num_iterations,&nbsp;learning_rate,&nbsp;print_cost)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Retrieve&nbsp;parameters&nbsp;w&nbsp;and&nbsp;b&nbsp;from&nbsp;dictionary&nbsp;&quot;parameters&quot;
&nbsp;&nbsp;&nbsp;&nbsp;w&nbsp;=&nbsp;parameters[&quot;w&quot;]
&nbsp;&nbsp;&nbsp;&nbsp;b&nbsp;=&nbsp;parameters[&quot;b&quot;]
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Predict&nbsp;test/train&nbsp;set&nbsp;examples&nbsp;(≈&nbsp;2&nbsp;lines&nbsp;of&nbsp;code)
&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction_test&nbsp;=&nbsp;predict(w,&nbsp;b,&nbsp;X_test)
&nbsp;&nbsp;&nbsp;&nbsp;Y_prediction_train&nbsp;=&nbsp;predict(w,&nbsp;b,&nbsp;X_train)
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;###&nbsp;END&nbsp;CODE&nbsp;HERE&nbsp;###
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Print&nbsp;train/test&nbsp;Errors
&nbsp;&nbsp;&nbsp;&nbsp;print(&quot;train&nbsp;accuracy:&nbsp;{}&nbsp;%&quot;.format(100&nbsp;-&nbsp;np.mean(np.abs(Y_prediction_train&nbsp;-&nbsp;Y_train))&nbsp;*&nbsp;100))
&nbsp;&nbsp;&nbsp;&nbsp;print(&quot;test&nbsp;accuracy:&nbsp;{}&nbsp;%&quot;.format(100&nbsp;-&nbsp;np.mean(np.abs(Y_prediction_test&nbsp;-&nbsp;Y_test))&nbsp;*&nbsp;100))
&nbsp;
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;d&nbsp;=&nbsp;{&quot;costs&quot;:&nbsp;costs,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Y_prediction_test&quot;:&nbsp;Y_prediction_test,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;Y_prediction_train&quot;&nbsp;:&nbsp;Y_prediction_train,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;w&quot;&nbsp;:&nbsp;w,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;b&quot;&nbsp;:&nbsp;b,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;learning_rate&quot;&nbsp;:&nbsp;learning_rate,
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&quot;num_iterations&quot;:&nbsp;num_iterations}
&nbsp;
&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;d
&nbsp;
d&nbsp;=&nbsp;model(train_set_x,&nbsp;train_set_y,&nbsp;test_set_x,&nbsp;test_set_y,&nbsp;num_iterations&nbsp;=&nbsp;2000,&nbsp;learning_rate&nbsp;=&nbsp;0.005,&nbsp;print_cost&nbsp;=&nbsp;True)</pre><p><br/></p>