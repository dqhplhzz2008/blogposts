---
ID: 3681
post_title: >
  吴恩达深度学习课程DeepLearning.ai笔记（5-1）
post_name: '%e5%90%b4%e6%81%a9%e8%be%be%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e8%af%be%e7%a8%8bdeeplearning-ai%e7%ac%94%e8%ae%b0%ef%bc%885-1%ef%bc%89'
author: 小奥
post_date: 2018-03-11 16:01:12
layout: post
link: >
  http://www.yushuai.me/2018/03/11/3681.html
published: true
tags:
  - 人工智能
  - 机器学习
  - 神经网络
categories:
  - Deep Learning
---
<p><strong><span style="font-size:24px;font-family:宋体">序列模型</span></strong></p><p><a></a><strong><span style="font-size:18px;font-family:宋体">1. Why sequence models?</span></strong></p><p style="text-indent:28px">在实际的生活中，很多东西都是序列信号，例如语音信号、音乐产生、情感分类、DNA序列分析等，这些虽然可以使用一般神经网络去完成，但就如同图像识别一样，那样去做太过于复杂，因此在这里引入了循环神经网络（RNN）。</p><p><strong><span style="font-size:18px;font-family:宋体">2. Notation</span></strong></p><p style="text-indent:28px">x：代表输入的语句，例如：Harry Potter and Herminone Granger invented a new spell.并且使用x<sup>&lt;t&gt;</sup>代表本句中第t个单词。</p><p style="text-indent:28px">y：代表输出结果。例如上句中对人名进行定位，则输出结果为“1 1 0 1 1 0 0 0 0 ”。同样的，y<sup>&lt;t&gt;</sup>代表本句中第t个单词的结果。</p><p style="text-indent:28px">Tx，Ty：分别代表输入和输出的长度。</p><p style="text-indent:28px">x<sup>(i)&lt;t&gt;</sup>，y<sup>(i)&lt;t&gt;</sup>：代表第i个样本的第t个单词以及对其的结果输出。</p><p style="text-indent:28px">利用one-hot编码，实现单词与符号的一一对应。每一个x实际上都是一个n维矩阵（n取决于句子的单词和符号数）。</p><p><strong><span style="font-size:18px;font-family:宋体">3. Recurrent Neural Network Model</span></strong></p><p style="text-indent:28px">在1中已经提到，SNN（标准神经网络）不适合应用在当前我们要处理的问题，现在将主要存在的问题表述如下：</p><p style="text-indent:28px">（1）在不同样本中，输入以及其输出的长度可能不相等；</p><p style="text-indent:28px">（2）不能共享从文本不同位置所学习到的特征。</p><p><strong>循环神经网络-前向传播</strong></p><p style="text-indent:28px">其结构如图1所示。在每一个时间步（Time step）中，循环神经网络会传递一个激活值到下一个时间步中，用于下一时间步的计算。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180311/1520783815405623.jpg" title="1520783815405623.jpg" alt="1520783815405623.jpg" width="579" height="220"/></p><p style="text-align:center">图1</p><p style="text-indent:28px">a<sup>&lt;0&gt;</sup>是一个需要编造一个激活值，通常输入一个零向量，有的研究人员会使用随机的方法对该初始激活向量进行初始化。各个参数的计算公式如图2所示。</p><p style="text-indent: 0em; text-align: center;"><img src="/wp-content/uploads/image/20180311/1520783845382444.jpg" title="1520783845382444.jpg" alt="2.jpg"/></p><p style="text-indent: 0em; text-align: center;">图2</p><p style="text-indent:28px">&nbsp;这是一个简化版本的公示形式。实际上W<sub>a</sub>=[W<sub>aa</sub>|W<sub>ax</sub>]。</p><p><strong>循环神经网络-反向传播</strong></p><p style="text-indent:28px">RNN的前向传播和反向传播结构图如图3所示。定义的损失函数与逻辑回归中的损失函数相同，在此不再累述。</p><p style="text-indent: 0em; text-align: center;"><img src="/wp-content/uploads/image/20180311/1520783880826862.jpg" title="1520783880826862.jpg" alt="1520783880826862.jpg" width="579" height="246"/></p><p style="text-align:center">图3</p><p style="text-indent:28px">下面简单列举下几种不同类型的RNN模型：</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180311/1520783896475712.jpg" title="1520783896475712.jpg" alt="1520783896475712.jpg" width="579" height="270"/></p><p style="text-align:center">图4</p><p><strong>梯度消失-GRU和LSTM</strong></p><p>3.1 梯度消失</p><p style="text-indent:28px">RNN在NLP中具有很大的应用价值，但是其存在一个很大的缺陷，那就是梯度消失的问题。例如下面的例句中：</p><p>The cat, which already ate ………..，was full；</p><p>The cats, which already ate ………..，were full.</p><p style="text-indent:28px">在这两个句子中，cat对应着was，cats对应着were，（中间存在很多很长省略的单词），句子中存在长期依赖（long-term dependencies），前面的单词对后面的单词有很重要的影响。但是我们目前所见到的基本的RNN模型，是不擅长捕获这种长期依赖关系的。</p><p style="text-indent:28px">对于梯度消失问题，在RNN的结构中是我们首要关心的问题，也更难解决；虽然梯度爆炸在RNN中也会出现，但对于梯度爆炸问题，因为参数会指数级的梯度，会让我们的网络参数变得很大，得到很多的Nan或者数值溢出，所以梯度爆炸是很容易发现的，我们的解决方法就是用梯度修剪，也就是观察梯度向量，如果其大于某个阈值，则对其进行缩放，保证它不会太大。</p><p>3.2 GRU（Gated Recurrent Unit）</p><p>RNN 单元：对于RNN的一个时间步的计算单元，在计算a&lt;t&gt;也就是下图右边的公式，能以左图的形式可视化呈现：</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180311/1520783936136351.jpg" title="1520783936136351.jpg" alt="1520783936136351.jpg" width="579" height="255"/></p><p style="text-align:center">图7</p><p>简化的GRU 单元：我们以时间步从左到右进行计算的时候，在GRU单元中，存在一个新的变量称为c，（代表cell）,作为“记忆细胞”，其提供了长期的记忆能力。GRU的可视化实现如图8所示。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180311/1520783958281164.jpg" title="1520783958281164.jpg" alt="1520783958281164.jpg" width="579" height="296"/></p><p style="text-align: center;">图8</p><p style="text-indent:28px">完整公式如图9所示。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180311/1520783975127265.jpg" title="1520783975127265.jpg" alt="1520783975127265.jpg" width="579" height="416"/></p><p style="text-align:center">图9</p><p style="text-indent:28px">第一个式子是指在每一个时间步上，给定一个候选值c˜&lt;t&gt;，用以替代原本的记忆细胞c&lt;t&gt;；</p><p style="text-indent:28px">第二个式子是代表<span style=";color:#454545;background:white">更新门，是一个</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">0-1</span><span style=";color:#454545;background:white">的值，用以决定是否对当前时间步的记忆细胞用候选值更新替代；</span></p><p style="text-indent:28px"><span style=";color:#454545;background:white">第三个式子是</span><span style=";color:#4F4F4F;background:white">以定每个时间步的候选值</span>；</p><p style="text-indent:28px"><span style=";color:#4F4F4F;background:white">第四个式子是</span><span style=";color:#454545;background:white">记忆细胞的更新规则，门控值处于</span><span style="font-family: &#39;Arial&#39;,sans-serif;color:#454545;background:white">0-1</span><span style=";color:#454545;background:white">之间，根据跟新公式能够有效地缓解梯度消失的问题；</span></p><p style="text-indent:28px"><span style=";color:#454545;background:white">第五个式子实际上记忆细胞输出的是在</span><span style="font-family: &#39;Arial&#39;,sans-serif;color:#454545;background:white">t</span><span style=";color:#454545;background:white">时间步上的激活值</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">a</span><span style=";color:#454545;background:white">。</span></p><p><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">3.3 LSTM</span></p><p style="text-indent:28px"><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">GRU</span><span style=";color:#454545;background:white">能够让我们在序列中学习到更深的联系，长短期记忆（</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">long short-term memory, LSTM</span><span style=";color:#454545;background:white">）对捕捉序列中更深层次的联系要比</span><span style="font-family: &#39;Arial&#39;,sans-serif;color:#454545;background:white">GRU</span><span style=";color:#454545;background:white">更加有效。</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">LSTM</span><span style=";color:#454545;background:white">的公式以及可视化图如图</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">10</span><span style=";color:#454545;background:white">所示。</span></p><p style="text-align:center"><span style=";color:#454545;background:white"><img src="/wp-content/uploads/image/20180311/1520783998670206.jpg" title="1520783998670206.jpg" alt="1520783998670206.jpg" width="579" height="295"/></span></p><p style="text-align:center"><span style=";color:#454545;background:white">图</span><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white">10</span></p><p></p><p><strong><span style="font-size:18px;font-family:宋体">4. Language model and sequence generation</span></strong></p><p>对于下面的例子：</p><p>The apple and pair salad.</p><p>The apple and pear salad.</p><p style="text-indent:28px">两句话有相似的发音，但是想表达的意义和正确性却不相同，如何让我们的构建的语音识别系统能够输出正确地给出想要的输出。也就是对于语言模型来说，从输入的句子中，评估各个句子中各个单词出现的可能性，进而给出整个句子出现的可能性。</p><p><strong>使用RNN来构建语言模型：</strong></p><p style="text-indent:28px">所谓训练集（Training Set）就是一大堆语言文本的集合（语料库）。Tokenize：将句子使用字典库标记化。</p><p style="text-indent:28px"><strong><span style="color:red">注意：未出现在字典库中的词使用“UNK”来表示。</span></strong></p><p style="text-indent:28px">第一步：使用零向量对输出进行预测，即预测第一个单词是某个单词的可能性；</p><p style="text-indent:28px">第二步：通过前面的输入，逐步预测后面一个单词出现的概率；</p><p style="text-indent:28px">训练网络：使用softmax损失函数计算损失，对网络进行参数更新，提升语言模型的准确率。</p><p><strong><span style="font-size:18px;font-family:宋体">5. Sampling novel sequences</span></strong></p><p style="text-indent:28px">在完成一个序列模型的训练之后，如果我们想要了解这个模型学到了什么，其中一种非正式的方法就是进行一次新序列采样（sample novel sequences）。</p><p style="text-indent:28px">对于一个序列模型，其模拟了任意特定单词序列的概率，如P(y&lt;1&gt;,<span style="font-family:&#39;MS Gothic&#39;">⋯</span>,y&lt;Ty&gt;)，而我们要做的就是对这个概率分布进行采样，来生成一个新的单词序列。</p><p style="text-indent:28px">如下面的一个已经训练好的RNN结构，我们为了进行采样需要做的：</p><p style="text-indent:28px">首先输入x&lt;1&gt;=0，a&lt;0&gt;=0在这第一个时间步，我们得到所有可能的输出经过softmax层后可能的概率，根据这个softmax的分布，进行随机采样，获取第一个随机采样单词y^&lt;1&gt;；</p><p style="text-indent:28px">然后继续下一个时间步，我们以刚刚采样得到的y^&lt;1&gt;作为下一个时间步的输入，进而softmax层会预测下一个输出y^&lt;2&gt;，依次类推；</p><p style="text-indent:28px">如果字典中有结束的标志如：“EOS”，那么输出是该符号时则表示结束；若没有这种标志，则我们可以自行设置结束的时间步。</p><p style="text-indent:28px">图5是基于单词的一个采样模型，图6是基于字母的采样模型。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180312/1520784086943175.jpg" title="1520784086943175.jpg" alt="1520784086943175.jpg" width="579" height="288"/></p><p style="text-align:center">图5</p><p style="text-align: center;"><img src="/wp-content/uploads/image/20180312/1520784057122128.jpg" title="1520784057122128.jpg" alt="1520784057122128.jpg" width="579" height="250"/></p><p style="text-align:center">图6</p><p style="text-indent:28px">但是基于字符的语言模型，一个主要的缺点就是我们最后会得到太多太长的输出序列，其对于捕捉句子前后依赖关系，也就是句子前部分如何影响后面部分，不如基于词汇的语言模型那样效果好；同时基于字符的语言模型训练代价比较高。所以目前的趋势和常见的均是基于词汇的语言模型。但随着计算机运算能力的增强，在一些特定的情况下，也会开始使用基于字符的语言模型。</p><p><strong><span style="font-size:18px;font-family:宋体">6. Bidirectional RNN</span></strong></p><p style="text-indent:28px">双向RNN则可以解决单向RNN存在的弊端。在BRNN中，不仅有从左向右的前向连接层，还存在一个从右向左的反向连接层。</p><p style="text-align:center"><img src="/wp-content/uploads/image/20180312/1520784037759136.jpg" title="1520784037759136.jpg" alt="1520784037759136.jpg" width="579" height="301"/></p><p style="text-align:center">图11</p><p>&nbsp;&nbsp;&nbsp; 其中，预测输出的值预测结果即有前向的信息，又有反向的信息。在NLP问题中，常用的就是使用双向RNN的LSTM。</p><p><span style="font-family:&#39;Arial&#39;,sans-serif;color:#454545;background:white"><br/></span><br/></p><p><br/></p>